\documentclass[11pt]{article}
\usepackage[a4paper,left=1.5cm,right=1.5cm,top=1.5cm,bottom=1.5cm]{geometry}
\usepackage{fancyhdr}
\usepackage{mleftright}
\usepackage{verbatim}
\renewcommand{\headrulewidth}{1pt}
\fancyhead[C]{\textsc{[LINMA2380] --- Homework 2}}
\fancyhead[L]{2 November 2020}
\fancyhead[R]{Group 02}

\usepackage{tikz}
\usepackage{pgfplots}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{csquotes}
\usepackage{mathtools,amssymb,amsthm}
\usepackage[binary-units=true,separate-uncertainty = true,multi-part-units=single]{siunitx}
\usepackage{float}
\usepackage[linktoc=all]{hyperref}
\hypersetup{breaklinks=true}
\graphicspath{{img/}}
\usepackage{caption}
\usepackage{textcomp}
\usepackage{array}
\usepackage{color}
\usepackage{tabularx,booktabs}
\usepackage{titlesec}
\usepackage{wrapfig}
\pagestyle{fancy}
\usepackage{mathrsfs}
\usepackage{bm}
\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}

\newcommand{\imag}{\mathrm{i}\mkern1mu} % Imaginary unit
\newcommand{\abs}[1]{\left\lvert#1\right\lvert}
\usepackage{listings}
\lstset{
    language=Python,
    numbers=left,
    numberstyle=\tiny\color{gray},
    basicstyle=\rm\small\ttfamily,
    keywordstyle=\bfseries\color{dkred},
    frame=single,
    commentstyle=\color{gray}=small,
    stringstyle=\color{dkgreen},
    %backgroundcolor=\color{gray!10},
    %tabsize=8, % Thank you Papa Torvalds
    %rulecolor=\color{black!30},
    %title=\lstname,
    breaklines=true,
    framextopmargin=2pt,
    framexbottommargin=2pt,
    extendedchars=true,
    inputencoding=utf8,
}

\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\vect}{vec}
\DeclareMathOperator{\newdiff}{d} % use \dif instead
\newcommand{\dif}{\newdiff\!}
\newcommand{\e}{\mathrm{e}}

\newcommand{\field}{\mathbb{F}} % field
\newcommand{\real}{\mathbb{R}} % real numbers
\newcommand{\complex}{\mathbb{C}} % complex numbers

\newcommand{\snorm}[1]{\norm{#1}_2} % spectral norm
\newcommand{\fnorm}[1]{\norm{#1}_F} % frobenius norm

\setcounter{MaxMatrixCols}{15}

\newcommand\undermat[2]{% http://tex.stackexchange.com/a/102468/5764
    \makebox[0pt][l]{$\smash{\underbrace{\phantom{%
                    \begin{matrix}#2\end{matrix}}}_{\text{$#1$}}}$}#2}

\begin{document}
\section*{Exercise A: Least square problems}
\subsection*{A1}
Minimising $\snorm{Ax - b}$ with respect to x means finding x such that the derivative of $\snorm{Ax - b}$ with respect to x is equal to zero. Equivalently, we can minimise the square of the norm which can be developed as follows:
\begin{align*}
    \snorm{Ax - b}^2 &= \snorm{Ax}^2 - 2 <Ax,b> + \snorm{b}^2 \\
    &= x^{T}A^{T}Ax - 2x^{T}A^{T}b + b^{T}b 
\end{align*}
The derivative with respect to x is: \\
\begin{align*}
    \frac{\partial \snorm{Ax - b}^2}{\partial x} &= \frac{\partial(x^{T}A^{T}Ax - 2x^{T}A^{T}b + b^{T}b)}{\partial x} \\
    &= \frac{\partial(x^{T}A^{T}Ax)}{\partial x} - \frac{\partial(2x^{T}A^{T}b)}{\partial x} + \frac{\partial(b^{T}b)}{\partial x} \\
    &= 2A^{T}Ax - 2A^{T}b + 0
\end{align*}
This derivative has to be equal to zero. Thus: \\
\begin{align*}
    \frac{\partial \snorm{Ax - b}^2}{\partial x} = 0 \\
    \Leftrightarrow 2A^{T}Ax - 2A^{T}b = 0 \\
    \Leftrightarrow 2A^{T}Ax = 2A^{T}b \\
    \Leftrightarrow A^{T}Ax = A^{T}b
\end{align*}

To demonstrate that when A has full column rank, i.e. rank(A)=n, the solution is unique, we first prove that Ker($A^{T}A$)=Ker(A): \\
\begin{align*}
    \forall x \in Ker(A): Ax = 0 \Leftrightarrow A^{T}Ax = 0 \Leftrightarrow x \in Ker(A^{T}A) \Rightarrow Ker(A) \subset Ker(A^{T}A)
\end{align*}

\begin{align*}
    \forall x \in Ker(A^{T}A): A^{T}Ax = 0 \Leftrightarrow x^{T}A^{T}Ax = 0 \Leftrightarrow \snorm{Ax} = 0 \Leftrightarrow Ax = 0 \Leftrightarrow x \in Ker(A) \\
    \Rightarrow Ker(A^{T}A) \subset Ker(A)
\end{align*}

According to the rank-nullity Theorem: \\
\begin{align*}
    rank(A) = n - dim(Ker(A)) \\
    yet: rank(A) = n \\
    \Rightarrow n = n - dim(Ker(A)) \\
    \Leftrightarrow dim(Ker(A))= 0 = dim(Ker(A^{T}A)) \\
    \Rightarrow Ker(A^{T}A) = \{ 0 \}
\end{align*}
As $Ker(A^{T}A)=\{ 0\}$ and $A^{T}A$ is square, we deduce $A^{T}A$ is invertible and it follows from Theorem 2.1 of the course notes that the solution of the system is unique.

\subsection*{A2}
Suppose the QR decomposition of $A$ is given by $\tiny{Q \begin{pmatrix}R\\0\end{pmatrix}}$, where $Q \in \real^{m\times m}$ is unitary and $R \in \real^{n\times n}$ is upper triangular. We will express the solution of 
\begin{equation}\label{eqA2}
    A^T Ax=A^T b
\end{equation} in terms of the QR decomposition of A.\\
Let $\tiny{R_f=\begin{pmatrix}R\\0\end{pmatrix}}$. We can rewrite equation \eqref{eqA2} as:
\begin{align*}
    (QR_f)^{T}QR_fx &= (QR_f)^Tb\\
    R_f^{T}Q^{T}QR_fx &= R_f^{T}Q^Tb
\end{align*}
As Q is unitary and hence $Q^TQ=I$, we have:
\begin{align*}
    \begin{pmatrix}
    R^T & 0
    \end{pmatrix}
    \begin{pmatrix}
    R\\ 0
    \end{pmatrix}x
    &=\begin{pmatrix}
    R^T & 0
    \end{pmatrix}
    Q^Tb
\end{align*}
If we call $\hat{Q}$ the matrix consisting of the first n columns of Q, equation \eqref{eqA2} becomes:
\begin{align*}
    R^TRx &= R^T\hat{Q}^Tb
\end{align*}
From theorem 2.8 of the course notes, we know that every matrix $A\in\complex^{m\times n}$ of full column-rank admits a factorization $A=Q_1R_1$ where $Q_1\in\complex^{m\times n}$ is an isometry and $R_1\in\complex^{n\times n}$ is an upper triangular matrix with positive diagonal. The matrix $Q_1$ corresponds to $\hat{Q}$ and $R_1$ simply to $R$. Hence we deduce that $R$ is invertible. This allows us to premultiply both sides of the equation by the inverse of $R^T$:
\begin{align*}
    R^{-T}R^TRx &= R^{-T}R^T\hat{Q}^Tb\\
    Rx &= \hat{Q}^Tb
\end{align*}
The solution of equation \eqref{eqA2} is therefore $x=R^{-1}\hat{Q}^Tb$ and the computation of the solution is reduced to the resolution of a single triangular system of linear equations (which can be solved efficiently using backward substitution).


\section*{Exercise B: Low-rank approximation}
\subsection*{B1}
For every matrix \(A \in \real^{m \times n}\), there exist unitary transformations \(U \in \real^{m \times m}\) and \(V \in \real^{n \times n}\) such that
\[
A = U \Sigma V^*, \quad \textnormal{where}\quad \Sigma = \left[\begin{array}{ccc|c}
\sigma_1 & & 0 & \\
& \ddots & & 0_{r \times (n-r)}\\
0 && \sigma_r & \\
\hline
& 0_{(m-r) \times r} & & 0_{(m-r) \times (n-r)}
\end{array}\right],
\]
with real positive singular values \(\sigma_1 \geqslant \dots \geqslant \sigma_r > 0\).

These singular values are unique: the intuition to see this is that the singular value decomposition is computed inductively, and that the unitary matrices preserve the norm.
By taking the property that \(\snorm{X} = \sigma_1\), this means that at every step of the decomposition, no matter what unitary transformations are chosen, the norm (and thus the maximal singular value of the submatrix we are working on) is the same.

Next, we show that the rank of a matrix is equal to its number of nonzero singular values.
\begin{proof}
    We first prove that if $A$ is a $K\times L$ matrix and $B$ is a full-rank $L\times L$ matrix then $\rank(AB)=\rank(A)$.\\
    Let $S_1$ be the space generated by the columns of $A$ and $S_2$ the space generated by the columns of $AB$.\\
    If $x\in S_1$ then there exists $y\in \real^{L\times 1}$ such that $x=Ay$. Considering $\hat{y}=B^{-1}y$ we note that $x=ABB^{-1}y=(AB)\hat{y}$ and so $x\in S_2$. If $x\in S_2$ then there exists $y\in \real^{L\times 1}$ such that $x=(AB)y$, we note that $x=A(By)$ and so $x\in S_2$. This shows that the spaces generated by the columns of $A$ and $AB$ coincide and hence $\rank(A)=\rank(AB)$.\\
    Similarly we prove that if $A$ is a $K\times L$ matrix and $B$ is a full-rank $K\times K$ matrix then $\rank(BA)=\rank(A)$.\\
    Let $S_1$ be the space generated by the rows of $A$ and $S_2$ the space generated by the rows of $BA$.\\
    If $x\in S_1$ then there exists $y\in \real^{1\times K}$ such that $x=yA$. Considering $\hat{y}=yB^{-1}$ we note that $x=yB^{-1}BA=\hat{y}(BA)$ and so $x\in S_2$. If $x\in S_2$ then there exists $y\in \real^{1\times K}$ such that $x=y(BA)$, we note $x=(yB)A$ and so $x\in S_1$. This shows that the spaces generated by the rows of $A$ and $AB$ coincide and hence $\rank(A)=\rank(BA)$.\\
	We know that the rank of a diagonal matrix is equal to the number of its nonzero entries.
	We also note that in the decomposition \(A = U \Sigma V^*\), \(U\) and \(V^*\) are of full-rank.
	Therefore, \(\rank(A) = \rank(\Sigma) = r\) where $r$ is the number of nonzero singular values.
\end{proof}

\subsection*{B2}
Let \(x \in \real^{m \times n}\) be such that \(\abs{X_{ij}} \leqslant \varepsilon\) for all \(i \in \{1, \dots, m\}\) and \(j \in \{1, \dots, n\}\).
Let \(\snorm{X}\) be the \(2\)-norm of \(X\) and let \(\fnorm{X}\) be its Frobenius norm.
We show that \(\snorm{X} \leqslant \fnorm{X} \leqslant \sqrt{mn} \varepsilon\).
\begin{proof}
	First, we show the first inequality.
	We know from the lecture notes that
	\begin{align*}
	\snorm{X} &= \sigma_{\textnormal{max}},\\
	\fnorm{X} &= \left[\sum_i \sigma_i\right]^{1/2},
	\end{align*}
	where \(\sigma_i\) are the singular values of \(X\).
	From this, it is immediately clear that \(\snorm{X} \leqslant \fnorm{X}\).
	
	Next, we use an equivalent form of the Frobenius norm to show the second inequality:
	\[
	\fnorm{X} = \left[\sum_{i, j} \abs{X_{ij}}^2 \right]^{1/2}.
	\]
	Knowing that \(\abs{X_{ij}} \leqslant \varepsilon\), it is immediate that \(\fnorm{X} \leqslant \left[\sum_{i, j} \varepsilon^2\right]^{1/2} = \left[mn \varepsilon^2\right]^{1/2} = \sqrt{mn} \varepsilon\).
	This concludes the proof.
\end{proof}

We also give an example where these bounds are tight.
Indeed, consider the matrix \(X = I_1 \in \real^{1 \times 1}\).
Clearly, we have \(\abs{X_{ij}} \leqslant \varepsilon = 1\) for all \(i, j\) (only one value is possible for each).
We know that the only singular value of this matrix is \(1\), and hence
\[
\snorm{X} = \fnorm{X} = \sqrt{1 \cdot 1} \varepsilon = 1 \varepsilon.
\]
\subsection*{B3}
We start by observing that if \(B = A + X\), then by Theorem~3.28 in the lecture notes, we can write
\begin{align*}
\sigma_{\min(m, n) - j + 1}(B) &= \min_{\mathcal{S}_j} \max_{x \in \mathcal{S}_j \setminus \{0\}} \frac{\snorm{B\bm{x}}}{\snorm{\bm{x}}}\\
&= \min_{\mathcal{S}_j} \max_{x \in \mathcal{S}_j \setminus \{0\}} \frac{\snorm{(A + X)\bm{x}}}{\snorm{\bm{x}}}\\
&\leqslant\min_{\mathcal{S}_j} \max_{x \in \mathcal{S}_j \setminus \{0\}} \left(\frac{\snorm{A\bm{x}}}{\snorm{\bm{x}}} + \frac{\snorm{X\bm{x}}}{\snorm{\bm{x}}}\right)\\
&\leqslant \sigma_{\min(m, n) - j + 1}(A) + \sigma_{\min(m, n) - j + 1}(X).\\
\intertext{However, we know that \(A\) has rank \(r\), and hence by the result of B1, we find that if \(\min(m, n) - j + 1 > r\), the singular value of \(A\) in the expression is zero, and hence that}
\sigma_{\min(m, n) - j + 1}(B) &\leqslant \sigma_{\min(m, n) - j + 1}(X)\\
&\leqslant \sqrt{mn} \varepsilon,
\end{align*}
where the last inequality is a consequence of B2.

A criterion that can then be used to estimate the rank \(r\) of \(A\) is then to take the smallest \(r\) such that \(\sigma_{r + 1}(B) \leqslant \sqrt{mn} \varepsilon\).
This is very similar to the description given on p.~60 of the lecture notes, concerning the numerical rank of the matrix \(A\). 

\section*{Exercise C: Realization theory}

In this last exercise, we are interested in finding an AR model corresponding to the data obtained during the covid pandemic. Indeed, we want to find the parameters $\alpha_i$ of :
\[y(t) = \alpha_0 + \sum_{i=1}^p \alpha_iy(t-i)
\]
For this, we will minimimize the squared error $f = \|y-\hat{y} \|_2$. (We have N+1 outputs)

\begin{comment}
We will differenciate this error with respect to the parameters to find the minima. This gives us:


\begin{align*}
f &= \|y-\hat{y} \|_2 = \sum_{t=0}^N (y(t) - \alpha_0 - \sum_{i=1}^p \alpha_iy(t-i))^2\\
\frac{\partial f}{\partial \alpha_0} &= -2 (\sum_{t=0}^N (y(t) - \alpha_0 - \sum_{i=1}^p \alpha_iy(t-i))) = 0\\ 
\frac{\partial f}{\partial \alpha_i} &= -y(t-i) (\sum_{t=0}^N (y(t) - \alpha_0 - \sum_{i=1}^p \alpha_iy(t-i)))=0 \qquad \qquad i = 1\dots p\\ 
\end{align*}
\end{comment}
Let's rewrite our problem as a system of linear equations to have a least squares problem.
\begin{comment}
\begin{align*}
\hat{y}= \begin{bmatrix}
\hat{y}(0)\\
\vdots\\
\hat{y}(N)\\
\end{bmatrix}= \underbrace{\begin{bmatrix}
1 & 0 & 0 &\dots & 0\\
1 & y(0) & 0& \dots &0\\
1 & y(1) & y(0)& \dots &0\\
\vdots&\vdots &\vdots&\ddots&\\
1 & y(p-1)& y(p-2)&\dots& y(0)\\
1 & y(p)&y(p-1)&\dots& y(1)\\
\vdots&\vdots &\vdots&&\vdots\\
1& y(N-1)&y(N-2)&\dots& y(N-p)\\
\end{bmatrix}}_A
\underbrace{\begin{bmatrix}
\alpha_0\\
\vdots\\
\alpha_p\\
\end{bmatrix}}_x\\
\end{align*}
\end{comment}
%---------NOT SURE about beginning of AR process------
%\begin{comment}
\begin{align*}
\begin{bmatrix}
\hat{y}(p)\\
\vdots\\
\hat{y}(N)\\
\end{bmatrix}= \begin{bmatrix}
1 & y(p-1)& y(p-2)&\dots& y(0)\\
1 & y(p)&y(p-1)&\dots& y(1)\\
\vdots&\vdots &\vdots&&\vdots\\
1& y(N-1)&y(N-2)&\dots& y(N-p)\\
\end{bmatrix}
\end{align*}
%\end{comment}
%-------------------
Note that we consider $\hat{y}(0) = y(0)\dots \hat{y}(p-1) = y(p-1)$ as initial conditions.
Thus our problem can be written as $\min_{x\in\mathbb{R}^{p+1}} \|Ax-y\|_2$.

From there, we can use what we obtained in question A2 to solve the problem with a QR decomposition and backward substitution.
\subsection*{Discussion}
\subsection*{Bonus question}


\end{document}