\documentclass[11pt]{article}
\usepackage[a4paper,left=1.5cm,right=1.5cm,top=1.5cm,bottom=1.5cm]{geometry}
\usepackage{fancyhdr}
\usepackage{mleftright}
\usepackage{verbatim}
\renewcommand{\headrulewidth}{1pt}
\fancyhead[C]{\textsc{[LINMA2380] --- Homework 2}}
\fancyhead[L]{2 November 2020}
\fancyhead[R]{Group 02}

\usepackage{tikz}
\usepackage{pgfplots}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{csquotes}
\usepackage{mathtools,amssymb,amsthm}
\usepackage[binary-units=true,separate-uncertainty = true,multi-part-units=single]{siunitx}
\usepackage{float}
\usepackage[linktoc=all]{hyperref}
\hypersetup{breaklinks=true}
\graphicspath{{img/}}
\usepackage{caption}
\usepackage{textcomp}
\usepackage{array}
\usepackage{color}
\usepackage{tabularx,booktabs}
\usepackage{titlesec}
\usepackage{wrapfig}
\pagestyle{fancy}
\usepackage{mathrsfs}
\usepackage{bm}
\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}

\newcommand{\imag}{\mathrm{i}\mkern1mu} % Imaginary unit
\newcommand{\abs}[1]{\left\lvert#1\right\lvert}
\usepackage{listings}
\lstset{
    language=Python,
    numbers=left,
    numberstyle=\tiny\color{gray},
    basicstyle=\rm\small\ttfamily,
    keywordstyle=\bfseries\color{dkred},
    frame=single,
    commentstyle=\color{gray}=small,
    stringstyle=\color{dkgreen},
    %backgroundcolor=\color{gray!10},
    %tabsize=8, % Thank you Papa Torvalds
    %rulecolor=\color{black!30},
    %title=\lstname,
    breaklines=true,
    framextopmargin=2pt,
    framexbottommargin=2pt,
    extendedchars=true,
    inputencoding=utf8,
}

\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\vect}{vec}
\DeclareMathOperator{\newdiff}{d} % use \dif instead
\newcommand{\dif}{\newdiff\!}
\newcommand{\e}{\mathrm{e}}

\newcommand{\field}{\mathbb{F}} % field
\newcommand{\real}{\mathbb{R}} % real numbers
\newcommand{\complex}{\mathbb{C}} % complex numbers

\newcommand{\snorm}[1]{\norm{#1}_2} % spectral norm
\newcommand{\fnorm}[1]{\norm{#1}_F} % frobenius norm

\setcounter{MaxMatrixCols}{15}

\newcommand\undermat[2]{% http://tex.stackexchange.com/a/102468/5764
    \makebox[0pt][l]{$\smash{\underbrace{\phantom{%
                    \begin{matrix}#2\end{matrix}}}_{\text{$#1$}}}$}#2}

\begin{document}
\section*{Exercise A: Least square problems}
\subsection*{A1}
Minimising $\snorm{Ax - b}$ with respect to x means finding x such that the derivative of $\snorm{Ax - b}$ with respect to x is equal to zero. Equivalently, we can minimise the square of the norm which can be developed as follows:
\begin{align*}
    \snorm{Ax - b}^2 &= \snorm{Ax}^2 - 2 <Ax,b> + \snorm{b}^2 \\
    &= x^{T}A^{T}Ax - 2x^{T}A^{T}b + b^{T}b 
\end{align*}
The derivative with respect to x is: \\
\begin{align*}
    \frac{\partial \snorm{Ax - b}^2}{\partial x} &= \frac{\partial(x^{T}A^{T}Ax - 2x^{T}A^{T}b + b^{T}b)}{\partial x} \\
    &= \frac{\partial(x^{T}A^{T}Ax)}{\partial x} - \frac{\partial(2x^{T}A^{T}b)}{\partial x} + \frac{\partial(b^{T}b)}{\partial x} \\
    &= 2A^{T}Ax - 2A^{T}b + 0
\end{align*}
This derivative has to be equal to zero. Thus: \\
\begin{align*}
    \frac{\partial \snorm{Ax - b}^2}{\partial x} = 0 \\
    \Leftrightarrow 2A^{T}Ax - 2A^{T}b = 0 \\
    \Leftrightarrow 2A^{T}Ax = 2A^{T}b \\
    \Leftrightarrow A^{T}Ax = A^{T}b
\end{align*}

To demonstrate that when A has full column rank, i.e. rank(A)=n, the solution is unique, we first prove that Ker($A^{T}A$)=Ker(A): \\
\begin{align*}
    \forall x \in Ker(A): Ax = 0 \Leftrightarrow A^{T}Ax = 0 \Leftrightarrow x \in Ker(A^{T}A) \Rightarrow Ker(A) \subset Ker(A^{T}A)
\end{align*}

\begin{align*}
    \forall x \in Ker(A^{T}A): A^{T}Ax = 0 \Leftrightarrow x^{T}A^{T}Ax = 0 \Leftrightarrow \snorm{Ax} = 0 \Leftrightarrow Ax = 0 \Leftrightarrow x \in Ker(A) \\
    \Rightarrow Ker(A^{T}A) \subset Ker(A)
\end{align*}

According to the rank-nullity Theorem: \\
\begin{align*}
    rank(A) = n - dim(Ker(A)) \\
    yet: rank(A) = n \\
    \Rightarrow n = n - dim(Ker(A)) \\
    \Leftrightarrow dim(Ker(A))= 0 = dim(Ker(A^{T}A)) \\
    \Rightarrow Ker(A^{T}A) = \{ 0 \}
\end{align*}
As $Ker(A^{T}A)=\{ 0\}$ and $A^{T}A$ is square, we deduce $A^{T}A$ is invertible and it follows from Theorem 2.1 of the course notes that the solution of the system is unique.

\subsection*{A2}
Suppose the QR decomposition of $A$ is given by $\tiny{Q \begin{pmatrix}R\\0\end{pmatrix}}$, where $Q \in \real^{m\times m}$ is unitary and $R \in \real^{n\times n}$ is upper triangular. We will express the solution of 
\begin{equation}\label{eqA2}
    A^T Ax=A^T b
\end{equation} in terms of the QR decomposition of A.\\
Let $\tiny{R_f=\begin{pmatrix}R\\0\end{pmatrix}}$. We can rewrite equation \eqref{eqA2} as:
\begin{align*}
    (QR_f)^{T}QR_fx &= (QR_f)^Tb\\
    R_f^{T}Q^{T}QR_fx &= R_f^{T}Q^Tb
\end{align*}
As Q is unitary and hence $Q^TQ=I$, we have:
\begin{align*}
    \begin{pmatrix}
    R^T & 0
    \end{pmatrix}
    \begin{pmatrix}
    R\\ 0
    \end{pmatrix}x
    &=\begin{pmatrix}
    R^T & 0
    \end{pmatrix}
    Q^Tb
\end{align*}
If we call $\hat{Q}$ the matrix consisting of the first n columns of Q, equation \eqref{eqA2} becomes:
\begin{align*}
    R^TRx &= R^T\hat{Q}^Tb
\end{align*}
From theorem 2.8 of the course notes, we know that every matrix $A\in\complex^{m\times n}$ of full column-rank admits a factorization $A=Q_1R_1$ where $Q_1\in\complex^{m\times n}$ is an isometry and $R_1\in\complex^{n\times n}$ is an upper triangular matrix with positive diagonal. The matrix $Q_1$ corresponds to $\hat{Q}$ and $R_1$ simply to $R$. Hence we deduce that $R$ is invertible. This allows us to premultiply both sides of the equation by the inverse of $R^T$:
\begin{align*}
    R^{-T}R^TRx &= R^{-T}R^T\hat{Q}^Tb\\
    Rx &= \hat{Q}^Tb
\end{align*}
The solution of equation \eqref{eqA2} is therefore $x=R^{-1}\hat{Q}^Tb$ and the computation of the solution is reduced to the resolution of a single triangular system of linear equations (which can be solved efficiently using backward substitution).


\section*{Exercise B: Low-rank approximation}
\subsection*{B1}
For every matrix \(A \in \real^{m \times n}\), there exist unitary transformations \(U \in \real^{m \times m}\) and \(V \in \real^{n \times n}\) such that
\[
A = U \Sigma V^*, \quad \textnormal{where}\quad \Sigma = \left[\begin{array}{ccc|c}
\sigma_1 & & 0 & \\
& \ddots & & 0_{r \times (n-r)}\\
0 && \sigma_r & \\
\hline
& 0_{(m-r) \times r} & & 0_{(m-r) \times (n-r)}
\end{array}\right],
\]
with real positive singular values \(\sigma_1 \geqslant \dots \geqslant \sigma_r > 0\).
These singular values are unique. Indeed, they can be derived from the eigenvalues of the hermitian matrix $A^*A$ (as explained on page 39 of the course notes) which are uniquely defined.
%These singular values are unique: the intuition to see this is that the singular value decomposition is computed inductively, and that the unitary matrices preserve the norm. By taking the property that \(\snorm{X} = \sigma_1\), this means that at every step of the decomposition, no matter what unitary transformations are chosen, the norm (and thus the maximal singular value of the submatrix we are working on) is the same.

Next, we show that the rank of a matrix is equal to its number of nonzero singular values.
\begin{proof}
    We first prove that if $A$ is a $K\times L$ matrix and $B$ is a full-rank $L\times L$ matrix then $\rank(AB)=\rank(A)$.\\
    Let $S_1$ be the space generated by the columns of $A$ and $S_2$ the space generated by the columns of $AB$.\\
    If $x\in S_1$ then there exists $y\in \real^{L\times 1}$ such that $x=Ay$. Considering $\hat{y}=B^{-1}y$ we note that $x=ABB^{-1}y=(AB)\hat{y}$ and so $x\in S_2$. If $x\in S_2$ then there exists $y\in \real^{L\times 1}$ such that $x=(AB)y$, we note that $x=A(By)$ and so $x\in S_2$. This shows that the spaces generated by the columns of $A$ and $AB$ coincide and hence $\rank(A)=\rank(AB)$.\\
    Similarly we prove that if $A$ is a $K\times L$ matrix and $B$ is a full-rank $K\times K$ matrix then $\rank(BA)=\rank(A)$.\\
    Let $S_1$ be the space generated by the rows of $A$ and $S_2$ the space generated by the rows of $BA$.\\
    If $x\in S_1$ then there exists $y\in \real^{1\times K}$ such that $x=yA$. Considering $\hat{y}=yB^{-1}$ we note that $x=yB^{-1}BA=\hat{y}(BA)$ and so $x\in S_2$. If $x\in S_2$ then there exists $y\in \real^{1\times K}$ such that $x=y(BA)$, we note $x=(yB)A$ and so $x\in S_1$. This shows that the spaces generated by the rows of $A$ and $AB$ coincide and hence $\rank(A)=\rank(BA)$.\\
	We know that the rank of a diagonal matrix is equal to the number of its nonzero entries.
	We also note that in the decomposition \(A = U \Sigma V^*\), \(U\) and \(V^*\) are of full-rank.
	Therefore, \(\rank(A) = \rank(\Sigma) = r\) where $r$ is the number of nonzero singular values.
\end{proof}

\subsection*{B2}
Let \(x \in \real^{m \times n}\) be such that \(\abs{X_{ij}} \leqslant \varepsilon\) for all \(i \in \{1, \dots, m\}\) and \(j \in \{1, \dots, n\}\).
Let \(\snorm{X}\) be the \(2\)-norm of \(X\) and let \(\fnorm{X}\) be its Frobenius norm.
We show that \(\snorm{X} \leqslant \fnorm{X} \leqslant \sqrt{mn} \varepsilon\).
\begin{proof}
	First, we show the first inequality.
	We know from the lecture notes that
	\begin{align*}
	\snorm{X} &= \sigma_{\textnormal{max}},\\
	\fnorm{X} &= \left[\sum_i \sigma_i\right]^{1/2},
	\end{align*}
	where \(\sigma_i\) are the singular values of \(X\).
	From this, it is immediately clear that \(\snorm{X} \leqslant \fnorm{X}\).
	
	Next, we use an equivalent form of the Frobenius norm to show the second inequality:
	\[
	\fnorm{X} = \left[\sum_{i, j} \abs{X_{ij}}^2 \right]^{1/2}.
	\]
	Knowing that \(\abs{X_{ij}} \leqslant \varepsilon\), it is immediate that \(\fnorm{X} \leqslant \left[\sum_{i, j} \varepsilon^2\right]^{1/2} = \left[mn \varepsilon^2\right]^{1/2} = \sqrt{mn} \varepsilon\).
	This concludes the proof.
\end{proof}

We also give an example where these bounds are tight.
Indeed, consider the matrix \(X = \varepsilon \in \real^{1 \times 1}\).
Clearly, we have \(\abs{X_{ij}} \leqslant \varepsilon \) for all \(i, j\) (only one value is possible for each).
We know that the only singular value of this matrix is \(\varepsilon\), and hence
\[
\snorm{X} = \fnorm{X} = \sqrt{1 \cdot 1} \varepsilon = 1 \varepsilon.
\]
\subsection*{B3}
We start by observing that if \(B = A + X\), then by Theorem~3.28 in the lecture notes, we can write
\begin{align*}
\sigma_{\min(m, n) - j + 1}(B) &= \min_{\mathcal{S}_j} \max_{x \in \mathcal{S}_j \setminus \{0\}} \frac{\snorm{B\bm{x}}}{\snorm{\bm{x}}}\\
&= \min_{\mathcal{S}_j} \max_{x \in \mathcal{S}_j \setminus \{0\}} \frac{\snorm{(A + X)\bm{x}}}{\snorm{\bm{x}}}\\
&\leqslant\min_{\mathcal{S}_j} \max_{x \in \mathcal{S}_j \setminus \{0\}} \left(\frac{\snorm{A\bm{x}}}{\snorm{\bm{x}}} + \frac{\snorm{X\bm{x}}}{\snorm{\bm{x}}}\right)\\
&\leqslant \sigma_{\min(m, n) - j + 1}(A) + \sigma_{\min(m, n) - j + 1}(X).\\
\intertext{However, we know that \(A\) has rank \(r\), and hence by the result of B1, we find that if \(\min(m, n) - j + 1 > r\), the singular value of \(A\) in the expression is zero, and hence that}
\sigma_{\min(m, n) - j + 1}(B) &\leqslant \sigma_{\min(m, n) - j + 1}(X)\\
&\leqslant \sqrt{mn} \varepsilon,
\end{align*}
where the last inequality is a consequence of B2.

A criterion that can then be used to estimate the rank \(r\) of \(A\) is then to take the smallest \(r\) such that \(\sigma_{r + 1}(B) \leqslant \sqrt{mn} \varepsilon\).
This is very similar to the description given on p.~60 of the lecture notes, concerning the numerical rank of the matrix \(A\). 

\section*{Exercise C: Realization theory}

In this last exercise, we are interested in finding an AR model corresponding to the data obtained during the covid pandemic. Indeed, we want to find the parameters $\alpha_i$ of:
\[y(t) = \alpha_0 + \sum_{i=1}^p \alpha_iy(t-i)
\]
%For this, we will minimimize the squared error $f = \|y-\hat{y} \|_2$ where $\hat{y}$ is the estimated output and $y$ the true output.
Let's rewrite the problem as a system of linear equations:
\begin{align}\label{system}
\begin{pmatrix}
1 & y(p-1)& y(p-2)&\dots& y(0)\\
1 & y(p)&y(p-1)&\dots& y(1)\\
\vdots&\vdots &\vdots&&\vdots\\
1& y(N-1)&y(N-2)&\dots& y(N-p)\\
\end{pmatrix}
\begin{pmatrix}
\alpha_0\\
\alpha_1\\
\vdots\\
\alpha_p
\end{pmatrix}
=
\begin{pmatrix}
y(p)\\
\vdots\\
y(N)\\
\end{pmatrix}
\end{align}
Note that we consider $\hat{y}(0) = y(0)\dots \hat{y}(p-1) = y(p-1)$ as initial conditions.\\ 
We want to solve the system \eqref{system} to obtain estimation of the parameters $\alpha_i$. We call $A$ the matrix and we observe that it usually has more lines than columns. Therefore, it is of interest to solve the system using the least squares method. To do so, we can use what we obtained in question A2 to solve the problem with a QR decomposition and backward substitution. Indeed, we must simply solve the triangular system :
\begin{equation*}
    Rx = \hat{Q}^Tb
\end{equation*}
where $\hat{Q}$ and $R$ are derived from the QR decomposition of $A$.\\
We consider two cases : the confinement mode and the social distancing mode. Moreover, for each cases, we divide the data set into a training set (70\% of the data) and a validation set (30\% of the data).\\
Considering one mode, the system \eqref{system} has to be solved using the data of the training set. Next, an estimated $y$ that we call $\hat{y}$ can be reconstructed using the AR model equation. The first $p$ components of $\hat{y}$ are given by the initial conditions and then the next components are constructed based on the previous calculated estimates:
\begin{equation*}
    \hat{y}(t)=\alpha_0+\sum_{i=1}^p \alpha_i\hat{y}(t-i)
\end{equation*}
%Similarly, $\hat{y}$ can be calculated on the validation set as well using the estimated parameters.\\
% QUESTION : est-ce qu'on utilise les y estimé pour les p premières valeurs de y_val ou alors les vraies valeurs ?
The error between the true $y$ and the estimated $\hat{y}$ can be computed on both the training and validation set. This gives an idea of the estimation quality.\\

Next we will show the results we obtained while varying the parameter p.
\newpage 

\paragraph{Confinement mode}
First we will study the confinement mode.

As you can see in Figure \ref{fig:Conf_train}, when the parameter p increases, the training error has a tendency to decrease. Indeed, the AR model that we obtain fits better and better our training set data.


Then in Figure \ref{fig:Conf_val}, we observe that the validation error decreases (approximatively) until p = 19. And from then on, the error rises again. Through this, we can understand that around p = 20, the model "overfits" our training set and so it gives a good approximation for the training set but gives a poor performance for the validation set.

\begin{figure}[h!]
\includegraphics[scale=0.25]{Conf_train.jpg}
\includegraphics[scale=0.25]{Conf_Err_Train.jpg}
\caption{Confinement Mode - Training set - Fitting to AR Models}
\label{fig:Conf_train}
\end{figure}

\begin{figure}[h!]
\includegraphics[scale=0.25]{Conf_val.jpg}
\includegraphics[scale=0.25]{Conf_Err_Val.jpg}
\caption{Confinement Mode - Validation set}
\label{fig:Conf_val}
\end{figure}

\newpage

\paragraph{Social Distancing mode}

In the social distancing mode, we can make the same observations (in Figures \ref{fig:SD_train} and \ref{fig:SD_val}) as in the previous mode : the error in the training set decreases when p increases except at some peaks.
For the validation set, we once again observe overfitting when p is higher than 19.
\begin{figure}[h!]
\includegraphics[scale=0.25]{SD_train.jpg}
\includegraphics[scale=0.25]{SD_Err_Train.jpg}
\caption{Social Distancing Mode - Training set - Fitting to AR Models}
\label{fig:SD_train}
\end{figure}

\begin{figure}[h!]
\includegraphics[scale=0.25]{SD_val.jpg}
\includegraphics[scale=0.25]{SD_Err_Val.jpg}
\caption{Social Distancing Mode - Validation set}
\label{fig:SD_val}
\end{figure}
\subsection*{Discussion}

In both modes, we have confirmed our intuition that the error in the training set would decrease. However, it is not monotonic and we still observe some peaks. One way to try to explain it is that when we solve the least squares problem, we minimize the error between the real values $y(t)$ and $\hat{y}(t) = f(y(t-1),...y(t-p))$ : $e_1 = \| y-\hat{y}\|_2$. But when we calculate the error of the training set, we consider $\tilde{y}(t) = f(\tilde{y}(t-1),... \tilde{y}(t-p))$ : $e_2 = \| y-\tilde{y}\|_2$, and so we propagate the errors along the time $t$. And clearly this last error $e_2$ is bigger than the first one $e_1$. So our intuition that the error should decrease monotonically only applies to the error $e_1$ (which we have indeed observed) and not to $e_2$.

Then, about the validation set errors, we can conclude that overfitting a training set doesn't yield good results for future predictions.

Note that in the analysis above, we only considered values of p such that the linear system of equations has more rows than columns. 
In the case that we have more columns than rows, our system is underdetermined and so solving the system, so we get an infinity number of solutions that are exact solutions (if there are no linear combinations in the rows of course) which means that our training error is null in theory (in practice we have errors of order $10^{-4}$, for confinement mode, and $10^{-9}$, for social distancing mode, due to numerical errors). For the validation error, we still have an overfitting so it is very big for both modes.

\subsection*{Bonus question}
If we approximate the model in real time with data that we receive at fixed interval, we will have to solve a new least squares problem everytime we want to include new data to improve the system. This can be expensive especially if the system is big.

\end{document}
