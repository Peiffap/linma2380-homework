\documentclass[11pt]{article}
\usepackage[a4paper,left=1.5cm,right=1.5cm,top=1.5cm,bottom=1.5cm]{geometry}
\usepackage{fancyhdr}
\usepackage{mleftright}
\usepackage{verbatim}
\renewcommand{\headrulewidth}{1pt}
\fancyhead[C]{\textsc{[LINMA2380] --- Homework 3}}
\fancyhead[L]{23 November 2020}
\fancyhead[R]{Group 02}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{csquotes}
\usepackage{mathtools,amssymb,amsthm}
\usepackage[binary-units=true,separate-uncertainty = true,multi-part-units=single]{siunitx}
\usepackage{float}
\usepackage[linktoc=all]{hyperref}
\hypersetup{breaklinks=true}
\graphicspath{{img/}}
\usepackage{caption}
\usepackage{textcomp}
\usepackage{array}
\usepackage{color}
\usepackage{tabularx,booktabs}
\usepackage{titlesec}
\pagestyle{fancy}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage[ruled,linesnumbered]{algorithm2e}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}

\newcommand\bovermat[2]{%
  \makebox[0pt][l]{$\smash{\overbrace{\phantom{%
    \begin{matrix}#2\end{matrix}}}^{\text{#1}}}$}#2}
    
\newcommand{\imag}{\mathrm{i}\mkern1mu} % Imaginary unit
\newcommand{\abs}[1]{\left\lvert#1\right\lvert}
\newcommand{\kp}{\otimes}
\DeclareMathOperator{\vect}{vec}

\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\newdiff}{d} % use \dif instead
\newcommand{\dif}{\newdiff\!}
\newcommand{\e}{\mathrm{e}}
\newcommand{\bo}{\mathcal{O}}

\DeclareMathOperator{\diag}{diag}

\newcommand{\field}{\mathbb{F}} % field
\newcommand{\real}{\mathbb{R}} % real numbers
\newcommand{\complex}{\mathbb{C}} % complex numbers

\newcommand{\snorm}[1]{\norm{#1}_2} % spectral norm
\newcommand{\fnorm}[1]{\norm{#1}_F} % frobenius norm

\setcounter{MaxMatrixCols}{15}

\newcommand\undermat[2]{% http://tex.stackexchange.com/a/102468/5764
	\makebox[0pt][l]{$\smash{\underbrace{\phantom{%
					\begin{matrix}#2\end{matrix}}}_{\text{$#1$}}}$}#2}

\begin{document}
\section*{Exercise A: Boundedness of trajectories and Lyapunov equation}
\subsection*{A1}
\begin{proof}
We use the transformation \(y(t) = T x(t) \iff x(t) = T^{-1} y(t)\), where \(T\) is the transformation matrix used in the Jordan decomposition of \(A\) (and thus always exists).
We then show that it necessarily leads to \(y_i(t)\) having a Jordan block as transition matrix.
\begin{align*}
    \dot{y}(t) &= T \dot{x}(t) \\
    &= T A x(t) \\
    &= T A T^{-1} y(t) \\
    &= \diag\left\{J_(\lambda_1), \dots, J_r(\lambda_r)\right\} y(t).
\end{align*}
For the last equality, we used the definition of the Jordan form, applicable because of our particular choice of \(T\).
This is the equation of a continuous-time linear system, and each \(y_i(t)\) has \(J_i(\lambda_i)\) as transition matrix.
\end{proof}

\subsection*{A2}
We assume that $A=J_n(\lambda)$, hence we can write
\begin{align*}
    x(t) &= e^{At}x(0)=e^{J_n(\lambda)t}x(0).
\intertext{We can use the result of question B4 from Homework 1, which develops $e^{J_n(\lambda)}$, to obtain}
    x(t) &= e^{J_n(\lambda)t}x(0)\\
    &= e^{\lambda t}\Bigg(I+\sum_{k=1}^{n-1}\frac{1}{k!}\big(J_n(0)\big)^k\Bigg)^t x(0)\\
    &= e^{\lambda I t} e^{J_n(0) t} x(0)\\
    &= e^{\lambda t} \Bigg(I+\sum_{k=1}^{n-1} \frac{1}{k!}\big(J_n(0) t\big)^k\Bigg) x(0)\\
    &= e^{\lambda t} \Bigg(I+\sum_{k=1}^{n-1} \frac{1}{k!}\big(J_n(0)\big)^k t^k\Bigg) x(0),
\end{align*}
where the third and fourth equalities come from the definition of the matrix exponential.
We then develop the terms in parentheses: 
\begin{align*}
    I+\sum_{k=1}^{n-1}\frac{1}{k!}\big(J_n(0)\big)^k t^k = I &+ \frac{t}{1!}
    \begin{pmatrix}
    0&1&&\\
    &\ddots&\ddots&&\\
    &&0&1&\\
    &&&0&1\\
    &&&&0
    \end{pmatrix} + \frac{t^2}{2!}
    \begin{pmatrix}
    0&0&1&\\
    &\ddots&\ddots&\ddots&\\
    &&0&0&1\\
    &&&0&0\\
    &&&&0
    \end{pmatrix}\\ \\
    &+ \dots +\frac{t^k}{k!}
    \begin{pmatrix}
    \bovermat{$k$ zeros}{0&\dots&0}&1&\\
    &\ddots&&\ddots&\ddots&\\
    &&\ddots&&\ddots&1\\
    &&&\ddots&&0\\
    &&&&\ddots&\vdots\\
    &&&&&0
    \end{pmatrix}
    +\dots+\frac{t^{n-1}}{(n-1)!}
    \begin{pmatrix}
    \bovermat{$n-1$ zeros}{0&\dots&0}&1\\
     &\ddots&&0\\
     & & \ddots & \vdots\\
     & & & 0\\
    \end{pmatrix},
\end{align*}
which can be written concisely as
\[
    I+\sum_{k=1}^{n-1}\frac{1}{k!}\big(J_n(0)\big)^k t^k=
    \begin{pmatrix}
    1 & t/1! & \dots & t^k/k! & \dots & t^{n-1}/(n-1)!\\
      & \ddots & \ddots &  & \ddots & \vdots\\
      & & 1 & t/1! & & t^k/k! &\\
      & & & \ddots & \ddots & \vdots\\
      & & & & 1 & t/1!\\
      & & & & & 1\\
    \end{pmatrix}.
\]
We deduce from the previous expression that for $i\in [n]$,
\begin{equation*}
    x_i(t)=e^{\lambda t} \sum^{n}_{j=i} \frac{1}{(j-i)!}t^{j-i}x_j(0).
\end{equation*}

\subsection*{A3}
The minimal polynomial of a matrix \(A \in \complex^{n \times n}\) is the polynomial
\[
m(\lambda) = \prod_i (\lambda - \lambda_i)^{k_i^*},
\]
where
\[
k_i^* = \max_{1 \leqslant j \leqslant n_i} k_{i_j}
\]
is the size of the largest Jordan block with eigenvalue \(\lambda_i\) and \(n_i\) is the number of Jordan blocks with eigenvalue \(\lambda_i\).

Simple eigenvalues thus have the property that the size of the largest Jordan block with that eigenvalue \(\lambda_i\) is \(1\) (i.e. \(k_i^* = 1\)).

\subsection*{A4}
\begin{proof}\leavevmode
\begin{itemize}
	\item \(\textnormal{1} \implies \textnormal{2}\).
	
	We start by applying A1, to obtain a trajectory \(y(t)\) with \(J(\lambda_i)\) as transition matrix.
	If all trajectories are bounded, then that means (from applying A2 on the Jordan transition matrix for \(y(t)\)) that
	\[
	\sup_{t \geqslant 0} \norm{y_i(t)} = \sup_{t \geqslant 0} \norm*{e^{\lambda_i t} \sum_{j = i}^n \frac{t^{j - i}}{(j - i)!} y_j(0)} < \infty \quad \textnormal{for all } i.
	\]
	Clearly, there are two ways to satisfy this condition (which must be satisfied for all \(i\)): either \(\Re(\lambda_i) < 0\) (in which case the exponential dominates any polynomial to its right), or \(\Re(\lambda_i) = 0\), and \(n = i\) (otherwise, the sum becomes an unbounded polynomial which is \textit{not} dominated by the exponential as the latter simply performs a rotation in the complex plane).
	We thus require that \(n = i\), as in this case the trajectory keeps its length (also known as marginal stability).
	We finally observe that requiring \(n = i\) corresponds to having the Jordan block for \(\lambda\) be of dimension \(1 \times 1\), and thus \(\lambda\) must be simple.
	\item \(\textnormal{2} \implies \textnormal{1}\).

	From A1, we know there exists a change of coordinates $y(t)=Tx(t)$ such that $y(t)$ can be decomposed as follows
	\[
	y(t) = \big[y_1(t)^\top, \dots, y_r(t)^\top\big]^\top,
	\]
	where each $y_i(t)$, $i \in [r]$, is the trajectory of a continuous-time linear dynamical system with a Jordan block as transition matrix.
	
	From A2, one can then obtain an expression for the components $y_i(t)$.
	If $\lambda_i$ is such that $\Re(\lambda_i)<0$, then from A2, we clearly see that the expression $y_i(t)$ tends to zero, as the exponential decreases more rapidly than the polynomial.
	If $\lambda_i$ is purely imaginary, i.e. $\Re(\lambda_i) = 0$, then by statement 2 we can assume it is simple and hence by A3 we know the size of the largest Jordan block with that eigenvalue is \(1\).
	The expression of $y_i(t)$ then simplifies to $y_i(t)=e^{\lambda t} y_i(0)$, which remains bounded as \(t\) goes to infinity. %% si une eigenvalue vaut 0 ca reste bounded mais ca tend pas vers 0 (pas asymptotiquement stable)
	
	This shows that if all eigenvalues are either in the open left-hand plane or imaginary and simple, then $y_i(t)$ and hence $\norm{y(t)}=\norm{Tx(t)}$ remain bounded.
	Finally, by virtue of the inversibility of \(T\), this shows that \(\sup_{t \geqslant 0} \norm{x(t)} < \infty\).\qedhere
\end{itemize}
\end{proof}

\subsection*{A5}
% not really sure this is what they're asking for
\begin{proof}
	One can show that the matrix \(P\) must exist by proving that for \(P = I_n\), the statement holds.
	Indeed, \(I_n\) is positive definite and Hermitian.
	Since \(D\) and \(D^*\) are diagonal, their sum is also diagonal.
	In fact,
	\[
	D + D^* = \mathop{\mathrm{diag}}(\lambda_1 + \lambda_1^*, \dots, \lambda_n + \lambda_n^*),
	\]
	where \(\lambda_i\) and \(\lambda_i^*\) are the eigenvalues of \(D\) and \(D^*\), respectively.
	We know that the imaginary parts of these eigenvalues cancel each other out, and that the real parts are the same (and are nonpositive), by virtue of the definition of the conjugate transpose.
	We thus get
	\[
	D^*P + PD = D^*I_n + I_nD = D^* + D = \mathop{\mathrm{diag}}(\lambda_1 + \lambda_1^*, \dots, \lambda_n + \lambda_n^*) \preceq 0,
	\]
	which proves the existence of such a positive definite Hermitian matrix \(P\).
\end{proof}

\subsection*{A6}
\begin{proof}
First, we develop $B=I\kp A^*+A^\top\kp I$:
\begin{align*}
    B=
    \begin{pmatrix}
    A^* & & \\
    &\ddots&\\
    & & A^*
    \end{pmatrix}+
    \begin{pmatrix}
    a_{11} I & \dots & a_{1n} I\\
    \vdots & \ddots & \vdots\\
    a_{n1} I & \dots & a_{nn} I\\
    \end{pmatrix}.
\end{align*}
We know $A\in\complex^{n\times n}$ is a Jordan block with eigenvalue $\lambda$, thus
\begin{align*}
    B&=
    \renewcommand{\arraystretch}{1.5}
    \begin{pmatrix}
    \big(J_n(\lambda)\big)^* & & \\
    &\ddots&\\
    & & \big(J_n(\lambda)\big)^*
    \end{pmatrix}+
    \begin{pmatrix}
    \lambda I & & &\\
    I & \ddots & &\\
    & \ddots & \ddots &\\
    & & I & \lambda I
    \end{pmatrix}\\
    &=\begin{pmatrix}
    \big(J_n(\lambda)\big)^*+\lambda I &  &  & \\
    I & \ddots & &\\
    & \ddots & \ddots &\\
    & & I & \big(J_n(\lambda)\big)^*+\lambda I
    \end{pmatrix}.
\end{align*}

This can then be rewritten as
\[
    B =\begin{pmatrix}
    \big(J_n(\lambda+\lambda^*)\big)^* & & & \\
    I & \ddots & &\\
    & \ddots & \ddots &\\
    & & I & \big(J_n(\lambda+\lambda^*)\big)^*
    \end{pmatrix}.
\]
The matrix \(B\) is lower-triangular and therefore its eigenvalues are the diagonal elements $\lambda+\lambda^*$.
Moreover, we know that $\Re(\lambda) = \Re(\lambda^*) <0$ and so $\lambda+\lambda^*=2 \Re(\lambda)<0$.
From this, we deduce that $B$ is negative definite and hence invertible.
It follows that the system $B\vect(P)=-\vect(Q)$ has a unique solution and consequently we conclude that \(P\) exists and is unique.
\end{proof}

Next we show that if $P\in\complex^{n\times n}$ satisfies $A^*P+PA=-Q$ then $P^*$ also satisfies $A^*P^*+P^*A=-Q$.
\begin{proof}
This can simply be proven by taking the transpose conjugate of both sides of the equation $A^*P+PA=-Q$ that \(P\) satisfies, taking into account that \(Q\) is Hermitian:
\begin{align*}
    \big(A^*P+PA\big)^*&=(-Q)^*\\
    \iff P^*A+A^*P^*&=-Q.
\end{align*}
This shows that $P^*$ also satisfies $A^*P^*+P^*A=-Q$.

Combining the last two results, we deduce that there always exists a unique Hermitian matrix $P$ satisfying $B\vect(P)=-\vect(Q)$.
\end{proof}

\begin{proof}
Finally, we want to show that $P$ is positive definite. We consider a trajectory $x(t)$ starting from $x(0)$. If we define $V\big(x(t)\big)=x(t)^*Px(t)$, we observe that:
\begin{align*}
    \dot{V}\big(x(t)\big)&=\dot{x}(t)^*P x(t)+x(t)^*P\dot{x}(t)\\
    &=x(t)^*A^*P x(t)+x(t)^*P A x(t)\\
    &=x(t)^*(A^*P+P A) x(t)\\
    &=x(t)^*(-Q) x(t)\\
    &< 0.
\end{align*}
Consequently, we find that for $t>0$ we have $x(t)^*P x(t) < x(0)^*P x(0)$.
We know from A2 that \(\lim_{t \to \infty} x(t) = 0\), as it decreases exponentially.
We thus have that \(V\big(x(t)\big)\) is always positive, and hence \(P\) is positive definite.
\end{proof}

\subsection*{A7}
\begin{proof}\leavevmode
\begin{itemize}
	\item \(\textnormal{2} \implies \textnormal{3}\).
	
	Properties of the eigenvalues are conserved when applying similarity transformations, hence without loss of generality, we can consider \(A\) to be in its Jordan normal form, with its simple eigenvalues in block $A_1$ (which is thus diagonal) and other eigenvalues in block $A_2$:
	\begin{align*}
	A \coloneqq \begin{pmatrix}
	A_1 & \\
	& A_2
	\end{pmatrix}.
	\end{align*}
	By A5, we then find a positive definite  Hermitian matrix $P_1$ such that $P_1A_1+A_1^*P_1=-Q_1$ and by A6 we find a positive definite Hermitian matrix $P_2$ such that $P_2A_2+A_2^*P_2=-Q_2$, with \(Q_1, Q_2\) both positive definite matrices.
	\begin{align*}
	P A + A^* P = 
	\begin{pmatrix}
	P_1 & \\
	& P_2
	\end{pmatrix}
	\begin{pmatrix}
	A_1 & \\
	& A_2
	\end{pmatrix}
	+
	\begin{pmatrix}
	A_1^* & \\
	& A_2^*
	\end{pmatrix}
	\begin{pmatrix}
	P_1 & \\
	& P_2
	\end{pmatrix}=-
	\begin{pmatrix}
	Q_1 & \\
	& Q_2
	\end{pmatrix} = -Q \preceq 0.
	\end{align*}
	This is precisely statement 3, as \(P_1, P_2\) being Hermitian implies \(P\) shares the same property.
	\item \(\textnormal{3} \implies \textnormal{1}\).
	
	We again consider \(V\big(x(t)\big)\) from A6.
	We can write, for any trajectory \(x(t)\),
	\[
	V\big(x(t)\big) = V\big(x(0)\big) + \int_0^{t} \dot{V}\big(x(\tau)\big) \dif \tau \leqslant V\big(x(0)\big).
	\]
	This means that the whole trajectory lies in \(\Big\{z \mid V(z) \leqslant V\big(x(0)\big)\Big\}\), which corresponds to a bounded ellipsoid for positive definite \(P\).\qedhere
\end{itemize}
\end{proof}

\subsection*{A8}
\begin{proof}\leavevmode
\begin{itemize}
	\item 3 \(\implies\) 3'.
	
	If statement 3 is satisfied, we know we have a positive definite Hermitian matrix $P$ such that $A^*P+PA \preceq 0$. We consider the real matrix $P_\textnormal{r}=P+P^*$ and we observe that
	\begin{align*}
	A^* P_\textnormal{r}+P_\textnormal{r} A &= A^*(P+P^*)+(P+P^*)A\\
	&= A^*P+A^*P^*+PA+P^*A\\
	&= (A^*P+PA)+(A^*P+PA)^* \preceq 0.
	\end{align*}
	We know the last expression is positive semidefinite as the conjugate transpose of a positive semidefinite matrix is positive semidefinite and the sum of positive semidefinite matrices is still positive semidefinite. Hence, $P_\textnormal{r}$ is a real matrix (by construction) satisfying the Lyapunov equation.
	\item 3' \(\implies\) 3.
	
	Clearly, if statement 3' is satisfied then so is statement 3, as real positive definite symmetric matrices are positive definite Hermitian matrices.
\end{itemize}
\end{proof}

\section*{Exercise B: Implementation}
\subsection*{B1}
First, we show that $I \kp A^{*} + A^\top \kp I = V (I \kp S^{*} + S^\top \kp I) V^{*}$, where \(V = \bar{U} \kp U\).

\begin{proof}
\begin{align*}
   V (I \kp S^{*} + S^\top \kp I) V^{*}
   &= (\bar{U} \kp U) (I \kp S^{*} + S^\top \kp I) (\bar{U} \kp U)^{*} \\
   &= \Big[(\bar{U} \kp U) (I \kp S^{*}) + (\bar{U} \kp U) (S^\top \kp I)\Big] (U^\top \kp U^{*}) \\
   &= \Big[(\bar{U} I \kp U S^{*}) + (\bar{U}S^\top \kp U I)\Big] (U^\top \kp U^{*}) \\
   &= (\bar{U} I \kp U S^{*}) (U^\top \kp U^{*}) + (\bar{U}S^\top \kp U I) (U^\top \kp U^{*}) \\
   &= (\bar{U} I U^\top \kp U S^{*} U^{*}) + (\bar{U}S^\top U^\top \kp U I U^{*}) \\
   &= \Big((U^{*^\top} I U^\top)\kp U (U S)^{*} \Big) + \Big((U^{*^\top} S^\top U^\top) \kp I \Big) \\
   &= \Big(\big((I^\top U^{*})^\top U^\top\big) \kp (U S U^{*})^{*} \Big) + \Big((S U^{*})^\top U^\top \kp I \Big) \\
   &= \Big((U I^\top U^{*})^\top \kp A^{*} \Big) + \Big((U S U^{*})^\top \kp I \Big)\\
   &= (I \kp A^{*} ) + (A^\top \kp I).\qedhere
\end{align*}
\end{proof}

Next, we prove that \(V\) is unitary:
\begin{proof}[\leavevmode]
\begin{align*}
   V V^{*} &= (\bar{U} \kp U) (\bar{U} \kp U)^{*} \\
   &= (\bar{U} \kp U) (U^\top \kp U^{*}) \\
   &= \bar{U} U^\top \kp U U^{*} \\
   &= (U \bar{U}^\top)^\top \kp I \\
   &= (U U^{*})^\top \kp I \\
   &= I^\top \kp I \\
   &= I \kp I \\
   &= I.\qedhere
\end{align*}
\end{proof}

We also show that $I \kp S^{*} + S^\top \kp I$ is lower triangular.
\begin{proof}
	This is apparent because \(S\) is upper triangular, thus $S^\top$ and $S^{*}$ are lower triangular.
	Their Kronecker products with the identity matrix remain lower triangular matrices and finally the sum of these products remains lower triangular as well.
\end{proof}

This system can then be solved as follows: we want to solve the system
\[
(I \kp A^* + A^\top \kp I) \vect(P) = -\vect(Q).
\]
By the first result proved in this question, that yields
\[
V (I \kp S^* + S^\top \kp I) V^* \vect(P) = -\vect(Q),
\]
which, by virtue of the second result, becomes
\[
(I \kp S^* + S^\top \kp I) V^* \vect(P) = - V^*\vect(Q).
\]

The third result of this question tells us that the system on the left is lower triangular.
This means it can be solved efficiently by forward substitution, leaving us with the value of \(V^* \vect(P)\).
Finally, we premultiply this by \(V\), and are left with \(\vect(P)\).

Next, we show that this method has a computational complexity in \(\bo(n^4)\).
\begin{proof}
A rudimentary complexity analysis of this method goes as follows (some negligible complexities are not considered as they are dominated by another complexity in the same step):
\begin{itemize}
	\item Compute the Schur decomposition of \(A = U S U^*\) in \(\bo(n^3)\).
	\item Compute \(V = \bar{U} \kp U\) and \(C = I \kp S^* + S^\top \kp I\) (Kronecker product, sum) in \(\bo(n^4)\) each.
	\item Compute \(D = -V^* \vect(Q)\) (matrix-vector product between \(n^2 \times n^2\) matrix and \(n^2 \times 1\) vector) in \(\bo(n^4)\).
	\item Solve \(C X = D\) (forward substitution on \(n^2 \times n^2\) system) in \(\bo(n^4)\).
	\item Compute \(\vect(P) = VX\) (matrix-vector product between \(n^2 \times n^2\) matrix and \(n^2 \times 1\) vector) in \(\bo(n^4)\).
\end{itemize}
As these computations are done sequentially, their total complexity is simply the dominant complexity of all the steps, i.e. \(\bo(n^4)\).
\end{proof}

\begin{comment}
Before presenting a method to solve $(I \kp A^{*} + A^\top \kp I) \vect(P) = - \vect(Q)$, let us introduce a lemma.
\begin{lemma}
\label{lemme}
Consider two matrices \(P, D\) partitioned in blocks according to
\begin{align*}
P = \begin{pmatrix} P_{1} \\ \vdots \\ P_{p-1} \end{pmatrix} \in \real^{N \times n_{p}}, \quad D = \begin{pmatrix} D_{1} \\ \vdots \\ D_{p-1} \end{pmatrix} \in \real^{N \times n_{p}},
\end{align*}
where $P_{j}, D_{j} \in \real^{n_{j} \times n_{p}}$ and $N = \sum_{j=1}^{p-1} n_{j}$.
Let $T \in \real^{n \times n}$ be a block triangular matrix containing P and D. For any $R_22 \in R^{n_{p}xn_{p}}$, if P satisfies the equation $D = T P + P R_{22}^{T}$, then $P_{j}$, p-1, p-2, ..., 1 satisfy 
\begin{align}
    T_{jj} P_{j} + P_{j} R_{22}^{T} = D_{j}^{\sim},
\end{align}
where $D_{j}^{\sim} = D_{j} - \sum_{i=j+1}^{p-1} T_{ji} P_{i}$.
\end{lemma}

The method to solve $(I \kp A^{*} + A^{T} \kp I) vec(P) = - vec(Q)$ is the following:\\
Since all matrices have a Schur decomposition: there exists matrices U and S such that:$A = U S U^{*}$ where U is an orthogonal matrix and $S \in R^{nxn}$ a block-triangular matrix where
\begin{align}
S = \begin{pmatrix} S_{11}  & \hdots & T_{1,r} \\  & \ddots & \vdots \\ & & T_{r,r}
\end{pmatrix}
\end{align} \label{matrix_S}
and $S_{jj} \in R^{n_{j}xn_{j}}$, $n_{j} \in \{1, 2\}$, j = 1, ..., r and $\sum_{j=1}^{r} n_{j} = n$. \\
The Lyapunov equation is multiplied from the right and left with U and $U^{*}$ respectively,
\begin{align}
- U Q U^{*} &= U A^{*} P U^{*} + U P A U^{*} \\
&= U A^{*} U^{*} U P U^{*} + U P U^{*} U A U^{*} \\
&= S Y + Y S^{*} \label{syys}
\end{align}
where $Y = U^{*} P U^{*}$. \\
Matrices and corresponding blocks are introduced such that
\begin{align}
    - U Q U^{*} = \begin{pmatrix} C_{11} & C_{12} \\ C_{21} & C_{22} \end{pmatrix}, Y = \begin{pmatrix} Z_{11} & Z_{12} \\ Z_{21} & Z{22} \end{pmatrix}, S = \begin{pmatrix} R_{11} & R_{12} \\ 0 & R_{22} \end{pmatrix}
\end{align}
where the blocks are such that $Z_{22}, C_{22}, S_{rr} = R_{22} \in R^{n_{r}xn_{r}}$ (the size of the last block of S). \\
This triangularized problem can be solved with backward substitution. \\
The following equations are obtained by separating thr blocks in the equation (\ref{syys})
\begin{align}
    C_{11} &= R_{11} Z_{11} + R_{12} Z_{21} + Z_{11} R_{11}^{T} + Z_{12} R_{12}^{T} \label{C11} \\
    C_{12} &= R_{11} Z_{12} + R_{12} Z_{22} + Z_{12} R_{22}^{T} \\
    C_{21} &= R_{22} Z_{21} + Z_{21} R_{11}^{T} + Z_{22} R_{12}^{T} \\
    C_{22} &= R_{22} Z_{22} + Z_{22} R_{22}^{T} \label{C22}
\end{align}

The last equation of size $n_{r} x n_{r}$ can be solved explicitly since $n_{r} \in \{1, 2\}$ thanks to the choice of block sizes:\\
if $n_{r} = 1$, $Z_{22}$ is scalar: $Z_{22} = \frac{C_{22}}{2 R_{22}}$ \label{Y22} \\
if $n_{r} = 2$, the equation \ref{C22} can be solved easily because it is a 2x2 Lyapunov equation. It can be solved with:$vec(Z_{22} = (I \kp R_{22} + R_{22} \kp I)^{-1} vec(C_{22})$ \label{vecY22}.\\
Thanks to the lemme \ref{lemme}, the now known matrix $Z_{22}$ can be insert in the others:\\
\begin{align}
C_{12}^{\sim} &= C_{12} - R_{12} Z_{22} = R_{11} Z_{12} + Z_{12} R_{22}^{T} \label{C12sim} \\ 
C_{21}^{\sim} &= C_{21}^{T} - R_{12} Z_{21}^{T} + Z_{21}^{T} R_{22}^{T} \label{C21sim}
\end{align}
$P_{j}$ can be computed explicitly from a small linear system $vec(P_j) = (I \kp S_{jj} + R_{22} \kp I)^{-1} vec(-Q_{j}^{\sim})$ \label{vecpj}. By solving \ref{vecpj} for j= p-1, ..., 1 for both equations of (\ref{C12sim}) and (\ref{C21}), solutions for $Z_{12}$ and $Z_{21}$ are obtained. Insertion of $Z_{12}, Z_{21}$ and $Z_{22}$ into (\ref{C11}) gives a new Lyapunov equation of size $n-n_{p}$ and the process can be repeated for the smaller matrix.
\begin{algorithm}[H]
Compute the real Schur decomposition [U, S] = schur(A) and establish $n_{1}, ..., n_{r}, S_{12}, S_{1r}, ..., S_{rr}$ with partitioning according to \ref{matrix_S}\\
Set $C = U (-Q) U^{*}$ \\
Set m = n \\
\For{k = r, ..., 1 do}
{Set $m = m - n_{k}$ \\
Partition the matrix C with $C_{11}, C_{12}, C_{21}, C_{22}$ according to C = \begin{pmatrix}
C_{11} & C_{12} \\ C_{21} & C_{22} \end{pmatrix} with $C_{22} \in R^{n_{k}xn_{k}}$ \\
Set $R_{22} = S_{kk}$ and $R_{11}$ = \begin{pmatrix} S_{11} & \hdots & S_{1,k-1} \\ & \ddots & \vdots \\ & & S_{k-1,k-1} \end{pmatrix}, $R_{12}$ = \begin{pmatrix} S_{1,k} \\ \vdots \\ S_{k-1,k} \end{pmatrix} \\
Solve \ref{C22} for $Z_{22} \in R^{n_{k}xn_{k}}$ using \ref{Y22} or \ref{vecY22} \\
Compute $C_{12}^{\sim}, C_{21}^{\sim}$ using \ref{C12sim} and \ref{C21sim} \\
Solve \ref{C12sim} and \ref{C21sim} for $Z_{12} \in R^{mxn_{k}}$ and $Z_{21} \in R^{n_{k}xm}$ using the lemme \ref{lemme} and \ref{vecpj} with p=j \\
Store Y(1:m,m+(1:$n_{k}$))=$Z_{12}$ \\
Store Y(m(1:$n_{k}$), 1:m) = $Z_{21}$ \\
Store Y(m+(1:$n_{k}$), m+(1:$n_{k}$))=$Z_{22}$ \\
Set $C = C_{11} - R_{12} Z_{21} - Z_{12} R_{12}^{T}$}\\
end
\Return{solution $P = U Y U^{*}$}
\caption{ }
\end{algorithm}
\end{comment}


\subsection*{B2}
We want to test the boundednesss of the trajectories defined by the systems with the three different matrices: $A_1$, $A_2$ and $A_3$ defined in the homework statement. For this purpose, we use statements 2 and 3 in the introduction of exercise A.
\subsubsection*{Statement 2}
To check whether statement 2 is satisfied or not, we looked at the vectors of eigenvalues $\Lambda_i$ and Jordan forms $J_i$ of the matrices:
\begin{align*}
    \Lambda_1 &=\begin{pmatrix}
    -0.5\\
    -0.5\\
    -1\\
    -2
    \end{pmatrix}, \quad
    J_1=\begin{pmatrix}
    -2 & 0 & 0 & 0\\
    0 &-1 & 0 & 0\\
    0 & 0 & -0.5 &1\\
    0 & 0 & 0 & -0.5
    \end{pmatrix}\\
    \Lambda_2 &=\begin{pmatrix}
    0\\
    -0.5\\
    -1\\
    -2
    \end{pmatrix}, \quad
    J_2=\begin{pmatrix}
    0 & 0 & 0 & 0\\
    0 &-0.5 & 0 & 0\\
    0 & 0 & -2 & 0\\
    0 & 0 & 0 & -1
    \end{pmatrix}\\
    \Lambda_3 &=\begin{pmatrix}
    0\\
    0\\
    -1\\
    -2
    \end{pmatrix}, \quad
    J_2=\begin{pmatrix}
    0 & 1 & 0 & 0\\
    0 & 0 & 0 & 0\\
    0 & 0 & -2 & 0\\
    0 & 0 & 0 & -1
    \end{pmatrix}.
\end{align*}
We observe that $A_1$ has eigenvalues such that $\Re(\lambda_{1, i})<0$ for all \(i\) and hence satisfies statement 2.
The matrix $A_2$ has eigenvalues such $\Re(\lambda_{2, i})<0$ for all \(i\) except for ($\lambda_{21}=0$), which is simple (the largest Jordan block associated to $\lambda_{21}$ has size \(1\)), so the statement is again verified.
The matrix $A_3$ has an eigenvalue ($\lambda_{3, 1} = \lambda_{3, 2} = 0$) with zero real part and associated to a Jordan block of size 2, which is thus not simple.
Therefore, it does not satisfy the statement. We conclude that the trajectory will necessarily remain bounded only for the first two systems.

\subsubsection*{Statement 3}
The use of statement 3 is less straightforward than that of the previous one. To know whether a positive definite Hermitian matrix $P$ satisfying the Lyapunov equation with $A_i$ exists, one can use the method described in the previous question to obtain $P$. To do so, we need to choose a positive definite Hermitian matrix $Q$. We first start by choosing $Q$ as being the identity matrix.

For the matrix $A_1$, we obtain:
\begin{align*}
P_1=
    \begin{pmatrix}
    735.2222& -696.2622& -398.0000&  377.9289\\
    -696.2622&  661.8022&  377.2533& -359.0156\\
    -398.0000&  377.2533&  216.1111& -205.1422\\
    377.9289& -359.0156& -205.1422&  195.3400\\
    \end{pmatrix}.
\end{align*}
The eigenvalues of $P_1$ are $1806.1$, $1.4$, $0.8$, and $0.2$, hence the $P_1$ is a positive definite Hermitian matrix and statement 3 holds.

Doing the exact same thing with the matrices $A_2$ and $A_3$ does not yield good results due to the zero eigenvalue. This is not surprising for \(A_3\), as due to the equivalence between the two statements, this is to be expected. However, the matrix \(P_2\) obtained with $A_2$ is unexpectedly
\begin{align*}
P_2 = 10^{15}
    \begin{pmatrix}
    6.3488&   -6.3488&   -3.1744&    3.1744\\
   -6.3488&    6.3488&    3.1744&   -3.1744\\
   -3.1744&    3.1744&    1.5872&   -1.5872\\
    3.1744&   -3.1744&   -1.5872&    1.5872
    \end{pmatrix},
\end{align*}
with eigenvalues $0$ and $1.5872\cdot 10^{16}$.
Computing $A_2^*P_2+P_2A_2$ gives
\begin{align*}
    A_2^*P_2+P_2A_2=
    \begin{pmatrix}
    88&   -40&   -24&     8\\
   -24&   -28&    -8&    18\\
   -24&    -8&    -8&     6\\
     8&    16&     2&   -16\\
    \end{pmatrix}\neq -Q = -I.
\end{align*}
As $P_2$ is not positive definite (because the computations are prone to numerical errors), we cannot conclude that statement 3 is satisfied for $A_2$ using the identity matrix for $Q$.

It is thus not guaranteed that for all positive semidefinite Hermitian matrices $Q$ we can numerically find a positive definite Hermitian matrix $P$ satisfying the Lyapunov equation. Finding a positive semidefinite matrix $Q$ such that there is an appropriate solution $P$ is tedious. On top of that, as we have seen with the computations using $Q=I$, numerical errors can further complicate this task. We tried to generate random positive definite matrices by generating a random matrix $Z$ and setting $Q=ZZ^*$, but this also did not yield satisfactory results. Finally, we note that such a matrix $Q$ does not even exist for $A_3$, as it does not fulfill statement 2.
\end{document}