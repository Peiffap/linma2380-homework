\documentclass[11pt]{article}
\usepackage[a4paper,left=1.5cm,right=1.5cm,top=1.5cm,bottom=1.5cm]{geometry}
\usepackage{fancyhdr}
\renewcommand{\headrulewidth}{1pt}
\fancyhead[C]{\textsc{[LINMA2380] --- Homework 1}}
\fancyhead[L]{12 October 2020}
\fancyhead[R]{Group 02}

\usepackage{tikz}
\usepackage{pgfplots}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{csquotes}
\usepackage{mathtools,amssymb,amsthm}
\usepackage[binary-units=true,separate-uncertainty = true,multi-part-units=single]{siunitx}
\usepackage{float}
\usepackage[linktoc=all]{hyperref}
\hypersetup{breaklinks=true}
\graphicspath{{img/}}
\usepackage{caption}
\usepackage{textcomp}
\usepackage{array}
\usepackage{color}
\usepackage{tabularx,booktabs}
\usepackage{titlesec}
\usepackage{wrapfig}
\pagestyle{fancy}
\usepackage{mathrsfs}
\usepackage{bm}
\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}

\newcommand{\imag}{\mathrm{i}\mkern1mu} % Imaginary unit
\newcommand{\abs}[1]{\left\lvert#1\right\lvert}
\usepackage{listings}
\lstset{
    language=Python,
    numbers=left,
    numberstyle=\tiny\color{gray},
    basicstyle=\rm\small\ttfamily,
    keywordstyle=\bfseries\color{dkred},
    frame=single,
    commentstyle=\color{gray}=small,
    stringstyle=\color{dkgreen},
    %backgroundcolor=\color{gray!10},
    %tabsize=8, % Thank you Papa Torvalds
    %rulecolor=\color{black!30},
    %title=\lstname,
    breaklines=true,
    framextopmargin=2pt,
    framexbottommargin=2pt,
    extendedchars=true,
    inputencoding=utf8,
}

\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\vect}{vec}
\DeclareMathOperator{\newdiff}{d} % use \dif instead
\newcommand{\dif}{\newdiff\!}
\newcommand{\e}{\mathrm{e}}

\newcommand{\field}{\mathbb{F}} % field
\newcommand{\complex}{\mathbb{C}} % complex numbers
\newcommand{\kp}{\otimes} % kronecker product

\setcounter{MaxMatrixCols}{15}

\newcommand\undermat[2]{% http://tex.stackexchange.com/a/102468/5764
    \makebox[0pt][l]{$\smash{\underbrace{\phantom{%
                    \begin{matrix}#2\end{matrix}}}_{\text{$#1$}}}$}#2}

\begin{document}
\section*{Exercise A: The Kronecker product}
\subsection*{A1}
The Kronecker product of two matrices \(A \in \field^{m \times n}\) and \(B \in \field^{p \times q}\) is the matrix of size \(mp \times nq\) whose elements are all possible products between the elements of \(A\) and \(B\) arranged in the following way:
\[
A \kp B \coloneqq \begin{bmatrix}
a_{11} B & \cdots & a_{1n}B \\
\vdots & \ddots & \vdots \\
a_{m1} B & \cdots & a_{mn} B
\end{bmatrix}
\]
 With $A = \begin{bmatrix}a_{11} & \cdots & a_{1n} \\
\vdots & \ddots & \vdots \\
a_{m1} & \cdots & a_{mn}
\end{bmatrix}, \quad B = \begin{bmatrix}b_{11} & \cdots & b_{1q} \\
\vdots & \ddots & \vdots \\
b_{p1} & \cdots & b_{pq}
\end{bmatrix}$

\subsection*{A2}
The Kronecker product is associative.
Let \(C \in \field^{s \times t}\) be a third matrix.
We show that \((A \kp B) \kp C = A \kp (B \kp C)\).

With $C = \begin{bmatrix}c_{11} & \cdots & c_{1t} \\
\vdots & \ddots & \vdots \\
c_{s1} & \cdots & c_{st}
\end{bmatrix}
$

\begin{proof}
\begin{align*}
(A \kp B) \kp C &= \begin{bmatrix}
a_{11} B & \cdots & a_{1n}B \\
\vdots & \ddots & \vdots \\
a_{m1} B & \cdots & a_{mn} B
\end{bmatrix} \kp C \\
&= \begin{bmatrix}
a_{11} b_{11} C & \cdots & a_{11} b_{1q} C & \cdots & a_{1n} b_{11} C & \cdots & a_{1n} b_{1q} C \\
\vdots & \ddots & \vdots & \ddots & \vdots & \ddots & \vdots \\
a_{11} b_{p1} C & \cdots & a_{11} b_{pq} C & \cdots & a_{1n} b_{p1} C & \cdots & a_{1n} b_{pq} C \\
\vdots & \ddots & \vdots & \ddots & \vdots & \ddots & \vdots \\
a_{m1} b_{11} C & \cdots & a_{m1} b_{1q} C & \cdots & a_{mn} b_{11} C & \cdots & a_{mn} b_{1q} C \\
\vdots & \ddots & \vdots & \ddots & \vdots & \ddots & \vdots \\
a_{m1} b_{p1} C & \cdots & a_{m1} b_{pq} C & \cdots & a_{mn} b_{p1} C & \cdots & a_{mn} b_{pq} C
\end{bmatrix}\\
&= A \kp (B \kp C).\qedhere
\end{align*}
\end{proof}

The Kronecker product is non-commutative; we show that \(A \kp B \neq B \kp A\)

\begin{proof}
We show a counterexample to the claim of commutativity.
Let
\[
A = \begin{bmatrix}
2 & 3 \\ 0 & 1
\end{bmatrix}, \quad, B = \begin{bmatrix}
0 & -1 \\ -1 & 1
\end{bmatrix}.
\]
In that case, we have
\[
A \kp B = \begin{bmatrix}
0 & -2 & 0 & -3 \\
-2 & 2 & -3 & 3 \\
0 & 0 & 0 & -1 \\
0 & 0 & -1 & 1
\end{bmatrix}, \quad B \kp A = \begin{bmatrix}
0 & 0 & -2 & -3 \\
0 & 0 & 0 & -1 \\
-2 & -3 & 2 & 3 \\
0 & -1 & 0 & 1
\end{bmatrix}.
\]
We see that \(A \kp B \neq B \kp A\).
\end{proof}

Finally, the set \(\field^{n \times n}\) equipped with the Kronecker product is not a group as it does not satisfy the closure property. Indeed, one can easily see that the Kronecker product of two matrices in \(\field^{n \times n}\) is an element of \(\field^{n^2 \times n^2}\).

\subsection*{A3}
Let \(A \in \field^{m \times n}\), \(B \in \field^{p \times q}\), \(C \in \field^{n \times r}\), and \(D \in \field^{q \times s}\),
\begin{equation*}
(A \kp B) (C \kp D)=(AC)\kp (BD)
\end{equation*}
\begin{proof}
We simply verify that
\begin{align*}
(A \kp B) (C \kp D) &= \begin{bmatrix}
a_{11} B & \cdots & a_{1n}B \\
\vdots & \ddots & \vdots \\
a_{m1} B & \cdots & a_{mn} B
\end{bmatrix} \begin{bmatrix}
c_{11} D & \cdots & c_{1r} D \\
\vdots & \ddots & \vdots \\
c_{n1} D & \cdots & c_{nr} D
\end{bmatrix}\\
&= \begin{bmatrix}
\sum_{k=1}^n a_{1k} c_{k1} BD & \cdots & \sum_{k=1}^n a_{1k} c_{kr} BD \\
\vdots & \ddots & \vdots \\
\sum_{k=1}^n a_{mk} c_{k1} BD & \cdots & \sum_{k=1}^n a_{mk} c_{kr} BD
\end{bmatrix}\\
&= AC \kp BD.
\end{align*}
\end{proof}

This allows us to say that (if \(A \in \field^{n \times n}\) and \(B \in \field^{m \times m}\) are nonsingular)
\[
(A \kp B)(A^{-1} \kp B^{-1}) = AA^{-1} \kp BB^{-1} = I_n \kp I_m = I_{nm},
\]
and hence that
\[
(A \kp B)^{-1} = A^{-1} \kp B^{-1}.\qedhere
\]

\subsection*{A4}
We first show the first property, P1:
\[
A^{\kp k} B^{\kp k} = (AB)^{\kp k}.
\]
\begin{proof}
By induction.
The base case is trivial:
\[
A^{\kp 1} B^{\kp 1} = AB = (AB)^{\kp 1}.
\]
Next, we assume the property holds for $k$, and we prove it for $k+1$:
\begin{align*}
A^{\kp k+1} B^{\kp k+1} &= \big(A^{\kp k} \kp A\big) \big(B^{\kp k} \kp B\big)\\
&\overset{\textnormal{A3}}{=}\big(A^{\kp k} B^{\kp k}\big) \kp AB\\
&= (AB)^{\kp k} \kp AB\\
&= (AB)^{\kp k+1}.\qedhere
\end{align*}
\end{proof}

Next, we show the second property, P2:
\[(A^{\kp k})^T = (A^T)^{\kp k}
\]
\begin{proof}
We start by proving an auxiliary lemma, L1.
\[
(A \kp B)^\top = \begin{bmatrix}
a_{11} B & \cdots & a_{1n}B \\
\vdots & \ddots & \vdots \\
a_{m1} B & \cdots & a_{mn} B
\end{bmatrix}^\top = \begin{bmatrix}
a_{11} B^\top & \cdots & a_{m1}B^\top \\
\vdots & \ddots & \vdots \\
a_{1n} B^\top & \cdots & a_{mn} B^\top
\end{bmatrix} = A^\top \kp B^\top.
\]

We then proceed by induction.
The base case is trivial as before:
\[
\big(A^{\kp 1}\big)^\top = A^\top = \big(A^\top\big)^{\kp 1}.
\]
Next, we assume the property holds for $k$, and we prove it for $k+1$:
\begin{align*}
\big(A^{\kp k+1}\big)^\top &= \big(A^{\kp k} \kp A\big)^\top\\
&\overset{\textnormal{L1}}{=} \big(A^{\kp k}\big)^\top \kp A^\top\\
&= \big(A^\top\big)^{\kp k} \kp A^\top\\
&= \big(A^\top\big)^{\kp k+1}.\qedhere
\end{align*}
\end{proof}

Finally, we show the following:
for any vector \(v \in \mathbb{R}^n\),
\[
\norm{v^{\kp k}} = \norm{v}^{k},
\]
where \(\norm{v} = \sqrt{v^\top v}\).
\begin{proof}
\begin{align*}
\norm{v^{\kp k}} &= \sqrt{\big(v^{\kp k}\big)^\top v^{\kp k}}\\
&\overset{\textnormal{P2}}{=} \sqrt{\big(v^\top\big)^{\kp k} v^{\kp k}}\\
&\overset{\textnormal{P1}}{=} \sqrt{(v^\top v)^{\kp k}}\\
&= \sqrt{\big(v^\top v\big)^k}\\
&= \Big(\sqrt{v^\top v}\,\Big)^k\\
&= \norm{v}^k,
\end{align*}
where the fourth equality follows from a simplification of the Kronecker product for scalars, and the fifth equality is a property of the square root.
\end{proof}

\subsection*{A5}
The determinant of a square matrix \(A \in \field^{n \times n}\) is given by
\[
\det(A) = \sum_{\bm{j}}(-1)^{t(\bm{j})} a_{1 j_1} \cdot a_{2 j_2} \cdots a_{n j_n},
\]
where the index vector \(\bm{j}\) constitutes a permutation of \(\{1, 2, \dots, n\}\), and \(t(\bm{j})\) denotes the parity of each quasi-diagonal.

First, we show that \(\det(A \kp I_m) = \det(A)^m\).
\begin{proof}
Laplace's theorem states that for a matrix \(B \in \field^{n\times n}\) and a \(p\)-tuple of rows $\bm{i}_p$, we have:
\begin{align*}
\det(B) &= \sum_{\bm{j}_p} B \binom{\bm{i}_p}{\bm{j}_p} B^c \binom{\bm{i}_p}{\bm{j}_p}.
\end{align*}
We apply this theorem to the matrix $B=A \kp I_m$ with the \(n\)-tuple \(\bm{i}_n = (1,m+1,2m+1,\dots,(n-1)m+1)\) (if \(n=1\) then the tuple is just \((1)\)).
For every \(n\)-tuple $\bm{j}_n$ that contains another index than those present in $\bm{i}_n$, the minor $B \binom{\bm{i}_n}{\bm{j}_n}$ is zero.
Indeed, if we consider a new matrix \(B'\) only containing the rows whose indices are in $\bm{i}_n$, we have:
\[
B' = \begin{bmatrix}
a_{11} & 0 & \cdots & 0 & a_{12} & 0 & \cdots & 0 & \cdots & a_{1n} & 0 & \cdots & 0\\
a_{21} & 0 & \cdots & 0 & a_{22} & 0 & \cdots & 0 & \cdots & a_{2n} & 0 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
\undermat{m \text{ columns}}{a_{n1} & 0 & \cdots & 0} & a_{n2} & 0 & \cdots & 0 & \cdots & a_{nn} & 0 & \cdots & 0\\
\end{bmatrix}.
\]
\newline
Thus if $\bm{j}_n$ contains any index not present in $\bm{i}_n$, a column of zeros is included leading to a zero minor.
We are left with
\begin{align}
\det(A \kp I_m) = \det(B) &= B \binom{\bm{i}_n}{\bm{i}_n} B^c \binom{\bm{i}_n}{\bm{i}_n}\\ \label{recc}
&= \det(A) \det(A \kp I_{m-1}).
\end{align}
The first term of \eqref{recc} can be easily found by inspecting \(B'\) and taking only columns whose indices are in $\bm{i}_n$.
The second term is derived by removing the rows and columns of $A\kp I_m$ whose indices are in $\bm{i}_n$, which gives the following matrix:
\begin{equation*}
\begin{bmatrix}
a_{11} I_{m-1} & \cdots & a_{1n} I_{m-1} \\
\vdots & \ddots & \vdots \\
a_{n1} I_{m-1} & \cdots & a_{nn} I_{m-1}
\end{bmatrix}.
\end{equation*}

From \eqref{recc}, we then conclude that \(\det(A \kp I_m) = \det(A)^m\).
\end{proof}

Similarly, we can also prove that \(\det(I_m \kp A) = \det(A)^m\).
\begin{proof}
    Indeed, if we take $B = I_m \kp A$ and $\bm{i_n} = (1, 2, \dots n)$, then we have :  
    \begin{equation*}B' = 
            \begin{bmatrix}
    a_{11} & a_{12} & \cdots & a_{1n} & 0 & \cdots &0\\
        a_{21} & a_{22} & \cdots & a_{2n} & 0 & \cdots &0\\
        \vdots & \vdots &\ddots & \vdots & \vdots &\ddots &\vdots\\
    a_{n1} & a_{n2} & \cdots & a_{nn} \undermat{(m-1)n \text{ columns}}{& 0 & \cdots &0}\\
    \end{bmatrix}
    \end{equation*}\\
    
    With the same argument as before, if $\bm{j}_n$ contains any index not present in $\bm{i}_n$, a column of zeros is included leading to a zero minor.
    
    We are left with
\begin{align*}
\det(I_m \kp A) = \det(B) &= B \binom{\bm{i}_n}{\bm{i}_n} B^c \binom{\bm{i}_n}{\bm{i}_n}\\ \label{recc}
&= \det(A) \det(I_{m-1}\kp A).
\end{align*}
Which leads us to \(\det(I_m \kp A) = \det(A)^m\).

\end{proof}
From this, we can deduce that for \(A \in \field^{n \times n}\) and \(B \in \field^{m \times m}\), \(\det(A \kp B) = \det(A)^m \det(B)^n\).
\begin{proof}
    We can write
    \begin{align*}
    A \kp B &= (A I_n) \kp (I_m B)\\
    &\overset{A3}{=} (A \kp I_m) (I_n \kp B).\\
    \intertext{Taking the determinant on both sides, and using the fact that 
        \(\det(AB) = \det(A)\det(B)\) (Exercise~1.18 in the lecture notes), we then get}
    \det(A \kp B) &= \det(A \kp I_m) \det(I_n \kp B)\\
    &= \det(A)^m \det(B)^n.\qedhere
    \end{align*}
\end{proof}

\subsection*{A6}
The rank of a matrix \(A \in \field^{m \times n}\) is equal to the largest size of its nonzero minors.
From this, we prove the following property: $\rank(A\kp B)=\rank(A)\rank(B)=\rank(B\kp A)$.

\begin{proof}
Let \(A \in \field^{m\times n}, B \in \field^{p\times q}\).

First, we note that $B\kp A$ can be obtained by permuting rows and columns of $A\kp B$.
As elementary operations do not affect the rank of a matrix, we deduce that $\rank(A\kp B) = \rank(B\kp A)$.

Let $R_1$ and $Q_1$ be products of elementary transformations such that
\begin{align*}
    R_1BQ_1 &=\begin{bmatrix}
    I_r & 0_{r\times(q-r)}\\
    0_{(p-r)\times r} & 0_{(p-r)\times (q-r)}
    \end{bmatrix}.
\end{align*}
By Theorem~1.8 of the lecture notes, we know such matrices exist.
The scalar $r$ is the rank of \(B\).

Next, we multiply on both sides the matrix $A\kp B$ by matrices with $R_1$ and $Q_1$ on the diagonal:
\begin{align*}
    &\begin{bmatrix}
    R_1 & &\\
    & \ddots & \\
    & & R_1
    \end{bmatrix}
    \begin{bmatrix}
    a_{11}B & \cdots & a_{1n}B\\
    \vdots & \ddots & \vdots\\
    a_{m1}B & \cdots & a_{mn}B
    \end{bmatrix}
    \begin{bmatrix}
    Q_1 & &\\
    & \ddots & \\
    & & Q_1
    \end{bmatrix}\\
    = &\begin{bmatrix}
    a_{11}R_1BQ_1 & \cdots & a_{1n}R_1BQ_1\\
    \vdots & & \vdots\\
    a_{m1}R_1BQ_1 & \cdots & a_{mn}R_1BQ_1
    \end{bmatrix}\\
    = &\begin{bmatrix}
    a_{11}I_r & 0_{r \times (q - r)} & \cdots & a_{1n}I_r & 0_{r \times (q-r)}\\
    0_{(p-r) \times r} & 0_{(p-r) \times (q-r)} & \cdots & 0_{(p-r) \times r} & 0_{(p-r) \times (q-r)}\\
    \vdots & \vdots & \ddots & \vdots & \vdots\\
    a_{m1}I_r & 0_{r \times (q - r)} & \cdots & a_{mn}I_r & 0_{r \times (q - r)}\\
    0_{(p-r) \times r} & 0_{(p-r) \times (q-r)}& \cdots & 0_{(p-r) \times r} & 0_{(p-r) \times (q-r)}\\
    \end{bmatrix}.
\end{align*}
By manipulating the last matrix with permutation matrices, the following matrix is obtained:
\[
    \begin{bmatrix}
    A & & & & &\\
    & \ddots & & & 0_{mr \times n(q-r)} &\\
    & & A & & & \\
    & & & & &\\
    & 0_{m(p-r) \times nr} & & & 0_{m(p-r) \times n(q-r)} &\\
    & & & & &
    \end{bmatrix},
\]
where the matrix \(A\) appears \(r\) times on the diagonal and the rest of the matrix is only zeros. 
Its rank is still equal to the rank of $A\kp B$ as only elementary operations have been applied.

Let $R_2$ and $Q_2$ be products of elementary transformations such that:
\begin{align*}
    R_2AQ_2 &=\begin{bmatrix}
    I_s & 0_{s\times(n-s)}\\
    0_{(m-s)\times s} & 0_{(m-s)\times (n-s)}
    \end{bmatrix}.
\end{align*}
We multiply on both sides the matrix obtained previously by matrices with $R_2$ and $Q_2$ on the diagonal :
\begin{align*}
    &\begin{bmatrix}
    R_2 & & & & &\\
    & \ddots & & & 0_{mr \times m(p-r)} &\\
    & & R_2 & & & \\
    & & & & &\\
    & 0_{m(p-r) \times nr} & & & 0_{m(p-r) \times m(p-r)} &\\
    & & & & &
    \end{bmatrix}
    \begin{bmatrix}
    A & & & & &\\
    & \ddots & & & 0_{nr \times n(q-r)} &\\
    & & A & & & \\
    & & & & &\\
    & 0_{n(q-r) \times nr} & & & 0_{n(q-r) \times n(q-r)} &\\
    & & & & &
    \end{bmatrix}\\
    &\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad
    \begin{bmatrix}
    Q_2 & & & & &\\
    & \ddots & & & 0_{nr \times n(q-r)} &\\
    & & Q_2 & & & \\
    & & & & &\\
    & 0_{n(q-r) \times nr} & & & 0_{n(q-r) \times n(q-r)} &\\
    & & & & &
    \end{bmatrix}\\
    = &\begin{bmatrix}
    R_2AQ_2 & & & & &\\
    & \ddots & & & 0_{mr \times n(q-r)} &\\
    & & R_2AQ_2 & & & \\
    & & & & &\\
    & 0_{m(p-r) \times nr} & & & 0_{m(p-r) \times n(q-r)} &\\
    & & & & &
    \end{bmatrix}\\
    = &\begin{bmatrix}
    I_s & 0_{s \times (n-s)} & & & & & \\
    0_{(m-s) \times s} & 0_{(m-s) \times (n-s)} & & & & &\\
    & & \ddots & & & & 0_{mr \times n(q-r)} &\\
    & & & I_s & 0_{s \times (n - s)} & & &\\
    & & & 0_{(m-s) \times s} & 0_{(m-s) \times (n-s)} & & &\\
    & & & & & & &\\
    & & 0_{m(p-r) \times nr} & & & & 0_{m(p-r) \times n(q-r)} &\\
    & & & & & & &
    \end{bmatrix}.
\end{align*}
As the identity matrix $I_s$ appears \(r\) times on the diagonal, we deduce that
\[
    \rank(A\kp B)=sr=\rank(A)\rank(B).\qedhere
\]
\end{proof}

\subsection*{A7}
We show that : $\vect(AXB)=\big(B^\top\kp A\big)\vect(X)$.
\begin{proof}
Let \(A \in \field^{m\times n}, B \in \field^{p\times q}\), and \(X \in \field^{n\times p}\).

We develop the right-hand side of the equality we want to prove:
\begin{align*}
    \big(B^\top\kp A\big)\vect(X) &=
    \begin{bmatrix}
    b_{11}A & b_{21}A & \cdots & b_{p1}A\\
    b_{12}A & b_{22}A & \cdots & \vdots \\
    \vdots & \vdots & \ddots & \vdots \\
    b_{1q}A & \cdots & \cdots & b_{pq}A
    \end{bmatrix}
    \begin{bmatrix}
    X_{:,1}\\
    X_{:,2}\\
    \vdots\\
    X_{:,p}
    \end{bmatrix}\\
    &=
    \begin{bmatrix}
    b_{11}AX_{:,1} + b_{21}AX_{:,2} + \cdots + b_{p1}AX_{:,p}\\
    b_{12}AX_{:,1} + \cdots + b_{p2}AX_{:,p}\\
    \vdots \\
    b_{1q}AX_{:,1} + \cdots + b_{pq}AX_{:,p}
    \end{bmatrix}.
\end{align*}
We recognize the elements of a product \(D\) of three matrices in vectorized form:
\[
    d_i=\sum^{p}_{r=1}b_{ri}\sum^{n}_{k=1}a_{rk}x_{kr},
\]
which shows \(D\) is simply \(AXB\), and hence proves \(\vect(AXB)=\big(B^\top\kp A\big)\vect(X)\).
\end{proof}

The proven equality can be used to solve the Sylvester equation: $AX+XA^\top=B$ where $X$ is the unknown.
Indeed, we can vectorize both sides of the equation:
\[
    \vect(AX) + \vect\big(XA^\top\big)=\vect(B).
\]
Then, we use some identity matrices to be able to apply the proven relation:
\begin{align*}
    \vect(B) = \vect(AX) + \vect\big(XA^\top\big) &= \vect(AXI) + \vect\big(IXA^\top\big)\\
    &=(I\kp A)\vect(X) + (A\kp I) \vect(X)\\
    &=(I\kp A + A\kp I) \vect(X).
\end{align*}
The term $\vect(X)$ can then be isolated:
\[
    \vect(X)=(I\kp A + A\kp I)^{-1}\vect(B).
\]
Finally, the matrix \(X\) can be simply reconstructed from $\vect(X)$.

\section{Exercise B: The matrix exponential}
\subsection*{B1}
If $\lambda\in \complex$ is an eigenvalue of \(A\) then $e^\lambda$ is an eigenvalue of $e^A$.
\begin{proof}
We know $\lambda$ is an eigenvalue of \(A\).
Hence $Av=\lambda v$ for some eigenvector $v$.
\begin{align*}
    e^Av&=\Bigg(I+\sum^{\infty}_{k=1}\frac{1}{k!}A^k\Bigg)v\\
    &=v+\sum^{\infty}_{k=1}\frac{1}{k!}A^kv\\
    &=v+\sum^{\infty}_{k=1}\frac{1}{k!}\lambda^kv\\
    &=\Bigg(\sum^{\infty}_{k=0}\frac{1}{k!}\lambda^k\Bigg)v\\
    &=e^\lambda v.
\end{align*}
The third line is derived using the equality $A^kv=\lambda^kv$ and the last line using the Taylor series of the exponential function.

This proves that $e^\lambda$ is an eigenvalue of $e^A$, with eigenvector \(v\).
\end{proof}

\subsection*{B2}
For any matrix $A\in \complex^{n\times n}$, $\rank(e^A)=n$.

\begin{proof}
The matrix $e^A$ is invertible and its inverse is $e^{â€”A}$.
In fact,
\[
    e^{A}e^{-A}=e^{-A}e^{A}=e^{A-A}=I.
\]
The second equality is valid since \(A\) and \(-A\) commute. Since $e^{A}$ is non-singular, its determinant is nonzero. Theorem 1.9 of the course notes states the rank of $e^A$ is equal to the largest size of its nonzero minors which in this case is the whole matrix. We deduce $\rank(e^A)=n$. 
\end{proof}

\subsection*{B3}
If \(A \in \complex^{n \times n}\) is skew-Hermitian, i.e. $A=-A^*$, then $e^A$ is unitary, i.e. $e^A(e^A)^*=I$.
\begin{proof}
We have:
\begin{align*}
    (e^A)^*&=\Bigg(I+\sum^{\infty}_{k=1}\frac{1}{k!}A^k\Bigg)^{*}\\
    &=\Bigg(I^{*}+\sum^{\infty}_{k=1}\frac{1}{k!}\big(A^k\big)^{*}\Bigg)\\
    &=\Bigg(I+\sum^{\infty}_{k=1}\frac{1}{k!}\big(A^{*}\big)^k\Bigg)\\
    &=e^{A^{*}}\\
    &=e^{-A},
\end{align*}
where the second and third lines are derived using the following properties:
\begin{itemize}
    \item $(A+B)^{*}=A^{*}+B^{*}$;
    \item $\big(A^k\big)^*=\big(A^*\big)^k$.
\end{itemize}
The fourth line comes from the definition of the exponential matrix and the last line from the fact that \(A\) is skew-Hermitian.

Finally can write :
\begin{align*}
    e^A(e^A)^*&=e^A(e^{-A})\\
    &=e^{A-A}\\
    &=I.
\end{align*}
The second line is valid since \(A\) and \(-A\) commute.
\end{proof}

\subsection*{B4}
We start by showing that \(\big(J_n(0)\big)^n=0\).
\begin{proof}
One trivially notes that \(A \coloneqq J_n(0) \in \complex^{n \times n}\) is strictly upper triangular, for any \(n \geqslant 1\).
We want to prove by induction on \(k \geqslant 1\) that \(a_{ij}^k = 0\) whenever \(i \leqslant j + k - 1\).

\begin{itemize}
\item When \(k = 1\), this is trivially true, as this precisely states that \(A\) is strictly upper triangular.

\item For the induction step, we fix \(k \geqslant 1\) and we suppose that \(a_{ij}^k = 0\) when \(i \leqslant j + k - 1\).

Fix \(i, j\) and suppose that \(i \leqslant j + (k + 1) - 1 = j + k \).
In that case,
\[
a_{ij}^{k+1} = \sum_{\ell = 0}^n a_{i \ell}^k a_{\ell j}.
\]
However, regarding the base case, we know that \(a_{\ell j} = 0\) whenever \(\ell \leqslant j\), so that
\[
a_{ij}^{k+1} = \sum_{\ell = j + 1}^n a_{i \ell}^k a_{\ell j}.
\]
Moreover, keeping in mind \(i \leqslant j + k \), we observe that if \(j+1 \leqslant \ell \leqslant n\), then
\[
i \leqslant j + k = (j + 1) + k - 1 \leqslant \ell + k - 1
\]
so that \(a_{i \ell}^k = 0\) by the induction hypothesis.
This allows us to conclude that \(a_{ij}^{k+1} = 0\) when \(i \leqslant j + (k+1) -1 \), as desired.
\end{itemize}
Finally, the result is obtained by taking the case \(k = n\), which tells us that \(a_{ij}^n = 0\) whenever \(i \leqslant j + n - 1\), but this is always true as \(i \leqslant n\) and \(j + n - 1 \geqslant n\) for all \(1 \leqslant i, j \leqslant n\).
This shows that \(A = \Big(J_n(0)\Big)^n = 0\).
\end{proof}

Next, we show that
\[
    e^{J_n(\lambda)}=e^{\lambda}\Bigg(I+\sum_{k=1}^{n-1}\frac{1}{k!}\big(J_n(0)\big)^k\Bigg).
\]
\begin{proof}
We note that $J_n(\lambda)=J_n(0)+\lambda I$.
Furthermore, $J_n(0)$ and $\lambda I$ commute as the identity matrix multiplied by a scalar commutes with every matrix.
Thus, we can write:
\begin{align*}
    e^{J_n(\lambda)}&=e^{\lambda I+J_n(0)}\\
    &=e^{\lambda I}e^{J_n(0)}.
\end{align*}
If we develop the last right-hand side and use the Taylor series expansion of the exponential, we obtain
\begin{align*}
    e^{J_n(0)}e^{\lambda I}&=\sum_{j=0}^{\infty}\frac{1}{j!}(\lambda I)^j\Bigg(\sum_{k=0}^{\infty}\frac{1}{k!}\big(J_n(0)\big)^k\Bigg)\\
    &=I\sum_{j=0}^{\infty}\frac{1}{j!}\lambda ^j\sum_{k=0}^{\infty}\frac{1}{k!}\big(J_n(0)\big)^k\\
    &=e^{\lambda}\Bigg(\sum_{k=0}^{\infty}\frac{1}{k!}\big(J_n(0)\big)^k\Bigg)\\
    &=e^{\lambda}\Bigg(I+\sum_{k=1}^{\infty}\frac{1}{k!}\big(J_n(0)\big)^k\Bigg).
\end{align*}
Putting the left-hand side in its original form, this is precisely what we were trying to prove.
\end{proof}

\subsection*{B5}
We want to show that for any matrices \(A, B \in \complex^{n \times n}\),
\[
    e^{A\kp I+I\kp B}=e^A\kp e^B.
\]

\begin{proof}
We first note that $A\kp I$ and $I\kp B$ commute.
Indeed, by using the mixed product property, we observe that
\begin{align*}
    (A\kp I)(I\kp B)=(AI)\kp(IB)=(I\kp B)(A\kp I).
\end{align*}
Hence we can write
\begin{equation*}
    e^{A\kp I+I\kp B}=e^{A\kp I}e^{I\kp B}.
\end{equation*}

Next, we develop the last equality using the definition of the matrix exponential:
\begin{align*}
    e^{A\kp I}e^{I\kp B}&=\sum_{k=0}^{\infty}\frac{1}{k!}(A\kp I)^k \sum_{j=0}^{\infty}\frac{1}{j!}(I\kp B)^j\\
    &=\sum_{k=0}^{\infty}\sum_{j=0}^{\infty}\frac{1}{k!}\,\frac{1}{j!}(A\kp I)^k (I\kp B)^j\\
    &=\sum_{k=0}^{\infty}\sum_{j=0}^{\infty}\frac{(k+j)!}{(k+j)!}\,\frac{1}{k!}\,\frac{1}{j!}(A\kp I)^k (I\kp B)^j\\
    &=\sum_{k=0}^{\infty}\sum_{j=0}^{\infty}\frac{1}{(k+j)!}\binom{k+j}{j}(A\kp I)^k (I\kp B)^j.
\end{align*}
Let $m=k+j$; we can then write this as
\begin{align*}
    &=\sum_{m=0}^{\infty} \sum_{j = 0}^m\frac{1}{m!}\binom{m}{j}(A\kp I)^{m-j} (I\kp B)^j\\
    &=\sum_{m=0}^{\infty}\frac{1}{m!}(A\kp I+I\kp B)^m\\
    &=e^{A\kp I+I\kp B},
\end{align*}
where the second equality comes from the binomial formula, which we can use because of the commutativity we observed earlier.
Note that the bounds on the sum indexed by \(j\) have changed; this is because of the change of variables and is consistent with the definition of \(m \geqslant j\).
\end{proof}
\end{document}
