\documentclass[11pt]{article}
\usepackage[a4paper,left=1.5cm,right=1.5cm,top=1.5cm,bottom=1.5cm]{geometry}
\usepackage{fancyhdr}
\renewcommand{\headrulewidth}{1pt}
\fancyhead[C]{\textsc{[LINMA2380] --- Homework 1}}
\fancyhead[L]{12 October 2020}
\fancyhead[R]{Group 02}

\usepackage{tikz}
\usepackage{pgfplots}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{csquotes}
\usepackage{mathtools,amssymb,amsthm}
\usepackage[binary-units=true,separate-uncertainty = true,multi-part-units=single]{siunitx}
\usepackage{float}
\usepackage[linktoc=all]{hyperref}
\hypersetup{breaklinks=true}
\graphicspath{{img/}}
\usepackage{caption}
\usepackage{textcomp}
\usepackage{array}
\usepackage{color}
\usepackage{tabularx,booktabs}
\usepackage{titlesec}
\usepackage{wrapfig}
\pagestyle{fancy}
\usepackage{mathrsfs}
\usepackage{bm}
\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}

\newcommand{\imag}{\mathrm{i}\mkern1mu} % Imaginary unit
\newcommand{\abs}[1]{\left\lvert#1\right\lvert}
\usepackage{listings}
\lstset{
	language=Python,
	numbers=left,
	numberstyle=\tiny\color{gray},
	basicstyle=\rm\small\ttfamily,
	keywordstyle=\bfseries\color{dkred},
	frame=single,
	commentstyle=\color{gray}=small,
	stringstyle=\color{dkgreen},
	%backgroundcolor=\color{gray!10},
	%tabsize=8, % Thank you Papa Torvalds
	%rulecolor=\color{black!30},
	%title=\lstname,
	breaklines=true,
	framextopmargin=2pt,
	framexbottommargin=2pt,
	extendedchars=true,
	inputencoding=utf8,
}

\DeclareMathOperator{\newdiff}{d} % use \dif instead
\newcommand{\dif}{\newdiff\!}
\newcommand{\e}{\mathrm{e}}

\newcommand{\field}{\mathbb{F}} % field
\newcommand{\kp}{\otimes} % kronecker product

% TODO
% - copy the statements (?)

\begin{document}
\section*{Exercise A: The Kronecker product}
\subsection*{A1}
The Kronecker product of two matrices \(A \in \field^{m \times n}\) and \(B \in \field^{p \times q}\) is the matrix of size \(mp \times nq\) whose elements are all possible products between the elements of \(A\) and \(B\) arranged in the following way:
\[
A \kp B \coloneqq \begin{bmatrix}
a_{11} B & \cdots & a_{1n}B \\
\vdots & \ddots & \vdots \\
a_{m1} B & \cdots & a_{mn} B
\end{bmatrix}.
\]

\subsection*{A2}
The Kronecker product is associative.
Let \(C \in \field^{s \times t}\) be a third matrix.
We show that \((A \kp B) \kp C = A \kp (B \kp C)\).
\begin{proof}
\begin{align*}
(A \kp B) \kp C &= \begin{bmatrix}
a_{11} B & \cdots & a_{1n}B \\
\vdots & \ddots & \vdots \\
a_{m1} B & \cdots & a_{mn} B
\end{bmatrix} \kp C \\
&= \begin{bmatrix}
a_{11} b_{11} C & \cdots & a_{11} b_{1q} C & \cdots & a_{1n} b_{11} C & \cdots & a_{1n} b_{1q} C \\
\vdots & \ddots & \vdots & \ddots & \vdots & \ddots & \vdots \\
a_{11} b_{p1} C & \cdots & a_{11} b_{pq} C & \cdots & a_{1n} b_{p1} C & \cdots & a_{1n} b_{pq} C \\
\vdots & \ddots & \vdots & \ddots & \vdots & \ddots & \vdots \\
a_{m1} b_{11} C & \cdots & a_{m1} b_{1q} C & \cdots & a_{mn} b_{11} C & \cdots & a_{mn} b_{1q} C \\
\vdots & \ddots & \vdots & \ddots & \vdots & \ddots & \vdots \\
a_{m1} b_{p1} C & \cdots & a_{m1} b_{pq} C & \cdots & a_{mn} b_{p1} C & \cdots & a_{mn} b_{pq} C
\end{bmatrix}\\
&= A \kp (B \kp C).\qedhere
\end{align*}
\end{proof}

The Kronecker is non-commutative; we show that \(A \kp B \neq B \kp A\)

\begin{proof}
We show a counterexample to the claim of commutativity.
Let
\[
A = \begin{bmatrix}
2 & 3 \\ 0 & 1
\end{bmatrix}, \quad, B = \begin{bmatrix}
0 & -1 \\ -1 & 1
\end{bmatrix}.
\]
In that case, we have
\[
A \kp B = \begin{bmatrix}
0 & -2 & 0 & -3 \\
-2 & 2 & -3 & 3 \\
0 & 0 & 0 & -1 \\
0 & 0 & -1 & 1
\end{bmatrix}, \quad B \kp A = \begin{bmatrix}
0 & 0 & -2 & -3 \\
0 & 0 & 0 & -1 \\
-2 & -3 & 2 & 3 \\
0 & -1 & 0 & 1
\end{bmatrix}.
\]
We see that \(A \kp B \neq B \kp A\).
\end{proof}

Finally, the set \(\field^{n \times n}\) equipped with the Kronecker product is a group by virtue of it being a field. % TODO check this

\subsection*{A3}
Let \(A \in \field^{m \times n}\), \(B \in \field^{p \times q}\), \(C \in \field^{n \times r}\), and \(D \in \field^{q \times s}\)
\begin{proof}
We simply verify that
\begin{align*}
(A \kp B) (C \kp D) &= \begin{bmatrix}
a_{11} B & \cdots & a_{1n}B \\
\vdots & \ddots & \vdots \\
a_{m1} B & \cdots & a_{mn} B
\end{bmatrix} \begin{bmatrix}
c_{11} D & \cdots & c_{1r} D \\
\vdots & \ddots & \vdots \\
c_{n1} D & \cdots & c_{nr} D
\end{bmatrix}\\
&= \begin{bmatrix}
\sum_{k=1}^n a_{1k} c_{k1} BD & \cdots & \sum_{k=1}^n a_{1k} c_{kr} BD \\
\vdots & \ddots & \vdots \\
\sum_{k=1}^n a_{mk} c_{k1} BD & \cdots & \sum_{k=1}^n a_{mk} c_{kr} BD
\end{bmatrix}\\
&= AC \kp BD.
\end{align*}
\end{proof}

This allows us to say that (if \(A \in \field^{n \times n}\) and \(B \in \field^{m \times m}\) are nonsingular)
\[
(A \kp B)(A^{-1} \kp B^{-1}) = AA^{-1} \kp BB^{-1} = I_n \kp I_m = I_{nm},
\]
and hence that
\[
(A \kp B)^{-1} = A^{-1} \kp B^{-1}.\qedhere
\]

\subsection*{A4}
We first show the first property, P1.
\begin{proof}
By induction.
The base case is trivial:
\[
A^{\kp 1} B^{\kp 1} = AB = (AB)^{\kp 1}.
\]
Next, we assume the property holds for \(k = n\), and we prove it for \(k = n+1\):
\begin{align*}
A^{\kp k+1} B^{\kp k+1} &= (A^{\kp k} \kp A) (B^{\kp k} \kp B)\\
&\overset{\textnormal{A3}}{=}(A^{\kp k} B^{\kp k}) \kp AB\\
&= (AB)^{\kp k} \kp AB\\
&= (AB)^{\kp k+1}.\qedhere
\end{align*}
\end{proof}

Next, we show the second property, P2.
\begin{proof}
We start by proving an auxiliary lemma, L1.
\[
(A \kp B)^\top = \begin{bmatrix}
a_{11} B & \cdots & a_{1n}B \\
\vdots & \ddots & \vdots \\
a_{m1} B & \cdots & a_{mn} B
\end{bmatrix}^\top = \begin{bmatrix}
a_{11} B^\top & \cdots & a_{m1}B^\top \\
\vdots & \ddots & \vdots \\
a_{1n} B^\top & \cdots & a_{mn} B^\top
\end{bmatrix} = A^\top \kp B^\top.
\]

We then proceed by induction.
The base case is trivial as before:
\[
(A^{\kp 1})^\top = A^\top = (A^\top)^{\kp 1}.
\]
Next, we assume the property holds for \(k = n\), and we prove it for \(k = n+1\):
\begin{align*}
(A^{\kp k+1})^\top &= (A^{\kp k} \kp A)^\top\\
&\overset{\textnormal{L1}}{=} (A^{\kp k})^\top \kp A^\top\\
&= (A^\top)^{\kp k} \kp A^\top\\
&= (A^\top)^{\kp k+1}.\qedhere
\end{align*}
\end{proof}

Finally, we show the following:
\[
\norm{v^{\kp k}} = \norm{v}^k.
\]
\begin{proof}
\begin{align*}
\norm{v^{\kp k}} &= \sqrt{(v^{\kp k})^\top v^{\kp k}}\\
&\overset{\textnormal{P2}}{=} \sqrt{(v^\top)^{\kp k} v^{\kp k}}\\
&\overset{\textnormal{P1}}{=} \sqrt{(v^\top v)^{\kp k}}\\
&= \sqrt{(v^\top v)^k}\\
&= \Big(\sqrt{v^\top v}\,\Big)^k\\
&= \norm{v}^k,
\end{align*}
where the fourth equality follows from a simplification of the Kronecker product for scalars, and the fifth equality is a property of the square root.
\end{proof}

\subsection*{A5}
The determinant of a square matrix \(A \in \field^{n \times n}\) as
\[
\det(A) = \sum_{\bm{j}}(-1)^{t(\bm{j})} a_{1 j_1} \cdot a_{2 j_2} \dots a_{n j_n},
\]
where the index vector \(\bm{j}\) constitutes a permutation of \(\{1, 2, \dots, n\}\), and \(t(\bm{j})\) denotes the parity of each quasi-diagonal.

Next, we show that \(\det(A \kp I_m) = \det(A)^m\).
\begin{proof}
The Laplace theorem states that for a matrix B of dimensions \(n\times n\) and a p-tuple of rows $\bm{i}_p$, we have :
\begin{align*}
\det(B) &= \sum_{\bm{j}_p} B {\bm{i}_p\choose \bm{j}_p} B^c {\bm{i}_p\choose \bm{j}_p}
\end{align*}
We apply this theorem to the matrix $B=A \kp I_m$ with the n-tuple $\bm{i}_n$ : $(1,m+1,2m+1,...,(n-1)m+1)$ (if n=1 then the tuple is just (1)). For every n-tuple $\bm{j}_n$ that contains another index than these present in $\bm{i}_n$, the minor $B {\bm{i}_n\choose \bm{j}_n}$ is zero. Indeed, if we consider only the rows whose indices are in $\bm{i}_n$, we have :
\begin{align*}
B'&=\begin{pmatrix}
a_{11} & 0 & \cdots & 0 & a_{12} & 0 & \cdots & 0 & \cdots & a_{1n} & 0 & \cdots & 0\\
a_{21} & 0 & \cdots & 0 & a_{22} & 0 & \cdots & 0 & \cdots & a_{2n} & 0 & \cdots & 0\\
\vdots & \vdots & & \vdots & \vdots & \vdots & & \vdots & & \vdots & \vdots & & \vdots \\
\undermat{\text{m columns}} {a_{n1} & 0 & \cdots & 0} & a_{n2} & 0 & \cdots & 0 & \cdots & a_{nn} & 0 & \cdots & 0 \\
\end{pmatrix}
\end{align*}\\

Thus if $\bm{j}_n$ contains any index not present in $\bm{i}_n$, a column full of zeros is included leading to a null minor. We are left with :
\begin{align}
\det(A \kp I_m) &= B {\bm{i}_n\choose \bm{i}_n} B^c {\bm{i}_n\choose \bm{i}_n}\\ \label{recc}
&= \det(A) \det(A \kp I_{m-1})
\end{align}
The first term of the second line can be easily found by inspecting B' and taking only columns whose indices are in $\bm{i}_n$. The second term is derived by removing the rows and columns of $A\kp I_m$ whose indices are in $\bm{i}_n$. This leads to :
\begin{equation*}
\begin{pmatrix}
a_{11} I_{m-1} & \cdots & a_{1n} I_{m-1} \\
\vdots & \ddots & \vdots \\
a_{n1} I_{m-1} & \cdots & a_{nn} I_{m-1}
\end{pmatrix}  
\end{equation*}

From \ref{recc}, we conclude that \(\det(A \kp I_m) = \det(A)^m\).
\end{proof}

\subsection*{A6}
The rank of a matrix \(A \in \field^{m \times n}\) is equal to the largest size of its nonzero minors.

Next, we prove the property : $\rank(A\kp B)=\rank(A)\rank(B)=\rank(B\kp A)$.

\begin{proof}
Let A be a matrix of dimension $n\times m$ and B a matrix of dimension $p\times q$.\\
First, we note that $B\kp A$ can be obtained by permuting rows and columns of $A\kp B$. As elementary operations do not affect the rank of a matrix, we deduce that $\rank(A\kp B)$ is equal to $\rank(B\kp A)$.\\
Let $R_1$ and $Q_1$ be products of elementary transformations such that :
\begin{align*}
    R_1BQ_1 &=\begin{bmatrix}
    I_r & 0_{r\times(q-r)}\\
    0_{(p-r)\times r} & 0_{(p-r)\times (q-r)}
    \end{bmatrix}
\end{align*}
By theorem 1.8 of the course, we know such matrices exist. The scalar $r$ is the rank of B.\\
Second, we multiply on both sides the matrix $A\kp B$ by matrices with $R_1$ and $Q_1$ on the diagonal :
\begin{align*}
    \begin{pmatrix}
    R_1 & &\\
    & \ddots & \\
    & & R_1
    \end{pmatrix}
    \begin{pmatrix}
    a_{11}B & \cdots & a_{1n}B\\
    \vdots & & \vdots\\
    a_{m1}B & \cdots & a_{mn}B
    \end{pmatrix}
    \begin{pmatrix}
    Q_1 & &\\
    & \ddots & \\
    & & Q_1
    \end{pmatrix}
    &= \begin{pmatrix}
    a_{11}R_1BQ_1 & \cdots & a_{1n}R_1BQ_1\\
    \vdots & & \vdots\\
    a_{m1}R_1BQ_1 & \cdots & a_{mn}R_1BQ_1
    \end{pmatrix}\\
    &= \begin{pmatrix}
    a_{11}I_r & \bm{0} & \cdots & a_{1n}I_r & \bm{0}\\
    \bm{0} & \bm{0} & \cdots & \bm{0} & \bm{0}\\
    \vdots & \vdots & & \vdots & \vdots\\
    a_{m1}I_r & \bm{0} & \cdots & a_{mn}I_r & \bm{0}\\
    \bm{0} & \bm{0} & \cdots & \bm{0} & \bm{0}\\
    \end{pmatrix}
\end{align*}
By manipulating the last matrix with permutation matrices, the following matrix is obtained :
\begin{equation*}
    \begin{psmallmatrix}
    A & & & & &\\
    & \ddots & & & &\\
    & & A & & & \\
    & & & 0 & &\\
    & & & & \ddots &\\
    & & & & & 0
    \end{psmallmatrix}
\end{equation*}\\
where the matrix A appears r times on the diagonal and the rest of the matrix is only zeros. Its rank is still equal to the rank of $A\kp B$ as only elementary operations have been applied.\\
Let $R_2$ and $Q_2$ be products of elementary transformations such that :
\begin{align*}
    R_2AQ_2 &=\begin{bmatrix}
    I_s & 0_{s\times(n-s)}\\
    0_{(m-s)\times s} & 0_{(m-s)\times (n-s)}
    \end{bmatrix}
\end{align*}
We multiply on both sides the matrix obtained previously by matrices with $R_2$ and $Q_2$ on the diagonal :
\begin{align*}
    \begin{psmallmatrix}
    R_2 & & & & & \\
    & \ddots & & & & \\
    & & R_2 & & &\\
    & & & 0 & &\\
    & & & & \ddots &\\
    & & & & & 0
    \end{psmallmatrix}
    \begin{psmallmatrix}
    A & & & & & &\\
    & \ddots & & & &\\
    & & A & & & &\\
    & & & 0 & & \\
    & & & & \ddots &\\
    & & & & & 0\\
    \end{psmallmatrix}
    \begin{psmallmatrix}
    Q_2 & & & & & \\
    & \ddots & & & & \\
    & & Q_2 & & &\\
    & & & 0 & &\\
    & & & & \ddots &\\
    & & & & & 0
    \end{psmallmatrix}
    &= \begin{psmallmatrix}
    R_2AQ_2 & & & & & \\
    & \ddots & & & & \\
    & & R_2AQ_2 & & &\\
    & & & 0 & &\\
    & & & & \ddots &\\
    & & & & & 0    
    \end{psmallmatrix}\\
    &= \begin{psmallmatrix}
    I_s & \bm{0} & & & & & \\
    \bm{0} & \bm{0} & & & & &\\
    & & \ddots & & & &\\
    & & & I_s & \bm{0} & & &\\
    & & & \bm{0} & \bm{0} & & &\\
    & & & & & 0 & &\\
    & & & & & & \ddots &\\
    & & & & & & & 0 
    \end{psmallmatrix}
\end{align*}
As the identity matrix $I_s$ appears r times on the diagonal, we deduce :
\begin{equation*}
    \rank(A\kp B)=sr=\rank(A)\rank(B)
\end{equation*}
\end{proof}

\subsection*{A7}
We show that : $\vect(AXB)=(B^{T}\kp A)\vect(X)$
\begin{proof}
Let A be a matrix of dimension $m\times n$, B a matrix of dimension $p\times q$ and X a matrix of dimension $n\times p$.\\
We develop the left term of the equality we want to prove :
\begin{align*}
    (B^{T}\kp A)\vect(X) &=
    \begin{pmatrix}
    b_{11}A & b_{21}A & \cdots & b_{p1}A\\
    b_{12}A & b_{22}A & & \\
    \vdots & & \ddots & \\
    b_{1q}A & & & b_{pq}A
    \end{pmatrix}
    \begin{pmatrix}
    X_{:,1}\\
    X_{:,2}\\
    \vdots\\
    X_{:,p}
    \end{pmatrix}\\
    &=
    \begin{pmatrix}
    b_{11}AX_{:,1} + b_{21}AX_{:,2} + \cdots + b_{p1}AX_{:,p}\\
    b_{12}AX_{:,1} + \cdots + b_{p2}AX_{:,p}\\
    \vdots \\
    b_{1q}AX_{:,1} + \cdots + b_{pq}AX_{:,p}
    \end{pmatrix}   
\end{align*}
We recognize the elements of a three matrices product :
\begin{equation*}
    d_{ij}=\sum^{p}_{r=1}b_{ij}\sum^{n}_{k=1}a_{rk}x_{kj}
\end{equation*}
\end{proof}
The proven equality can be used to solve the Sylvester equation : $AX+XA^{T}=B$ where $X$ is the unknown. Indeed, we can vectorize both sides of the equation :
\begin{equation*}
    \vect(AX) + \vect(XA^{T})=\vect(B)
\end{equation*}
Then we use some identity matrices to be able to apply the proven relation :
\begin{align*}
    \vect(AXI) + \vect(IXA^{T})&=\vect(B)\\
    (I\kp A)\vect(X) + (A\kp I) \vect(X)&=\vect(B)\\
    (I\kp A + A\kp I) \vect(X)&=\vect(B)
\end{align*}
The term $\vect(X)$ can be isolated :
\begin{equation*}
    \vect(X)=(I\kp A + A\kp I)^{-1}\vect(B)
\end{equation*}
Finally, the matrix X can be simply deduced from $\vect(X)$.

\section{Exercise B: The matrix exponential}
\subsection*{B1}
If $\lambda\in \complex$ is an eigenvalue of A then $e^\lamda$ is an eigenvalue of $e^A$.
\begin{proof}
We know $\lambda$ is an eigenvalue of A. Hence $Av=\lambda v$ for some eigenvector $v$.
\begin{align*}
    e^Av&=\Bigg(I+\sum^{\infty}_{k=1}\frac{1}{k!}A^k\Bigg)v\\
    &=v+\sum^{\infty}_{k=1}\frac{1}{k!}A^kv\\
    &=v+\sum^{\infty}_{k=1}\frac{1}{k!}\lambda^kv\\
    &=\Bigg(\sum^{\infty}_{k=0}\frac{1}{k!}\lambda^k\Bigg)v\\
    &=e^\lambda v
\end{align*}
The third line is derived using the equality $A^kv=\lambda^kv$ and the last line using the Taylor series of the exponential function.\\
This proves that $e^\lambda$ is an eigenvalue of $e^A$ with the eigenvector v.
\end{proof}

\subsection*{B2}
% TODO
\subsection*{B3}
If A is skew-Hermitian, i.e. $A=-A^{*}$, then $e^A$ is unitary, i.e. $e^A(e^A)^{*}=I)$.
\begin{proof}
We have :
\begin{align*}
    (e^A)^{*}&=\Bigg(I+\sum^{\infty}_{k=1}\frac{1}{k!}A^k\Bigg)^{*}\\
    &=\Bigg(I^{*}+\sum^{\infty}_{k=1}\frac{1}{k!}(A^k)^{*}\Bigg)\\
    &=\Bigg(I+\sum^{\infty}_{k=1}\frac{1}{k!}(A^{*})^k\Bigg)\\
    &=e^{A^{*}}\\
    &=e^{-A}
\end{align*}
The second and third lines are derived using the following properties :
\begin{itemize}
    \item $(A+B)^{*}=A^{*}+B^{*}$
    \item $(A^k)^*=(A^*)^k$
\end{itemize}
The fourth line comes from the definition of the exponential matrix and the last line from the fact that A is skew-Hermitian.\\
Finally can write :
\begin{align*}
    e^A(e^A)^{*}&=e^A(e^{-A})\\
    &=e^{A-A}\\
    &=I
\end{align*}
The second line is valid since A and -A commute.
\end{proof}

\subsection*{B4}
% TODO
\subsection*{B5}
% TODO
\end{document}
